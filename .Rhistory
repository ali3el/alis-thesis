rulefit_result
rulefit_result <- collect_metrics(rulefit_fit_b) |>
mutate(model = "RuleFit")
rulefit_result
log_b_acc <- logistic_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Logistic KS")
dtree_b_acc <- dtree_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Decision Tree KS")
knn_b_acc <- knn_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "KNN KS")
nbayes_b_acc <- nbayes_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Naive Bayes KS")
nbayes_result <- collect_metrics(nbayes_fit_b) |>
mutate(model = "Naive Bayes")
nbayes_result
log_b_acc <- logistic_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Logistic M1")
dtree_b_acc <- dtree_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Decision Tree M1")
knn_b_acc <- knn_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "KNN M1")
nbayes_b_acc <- nbayes_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Naive Bayes M1")
rulefit_b_acc <- rulefit_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "RuleFit M1")
table_a_accuracy <- bind_rows(log_b_acc, dtree_b_acc, knn_b_acc, nbayes_b_acc, rulefit_b_acc) |>
select(model, mean, std_err, n) |>
arrange(mean) |>
distinct()
table_b_accuracy <- bind_rows(log_b_acc, dtree_b_acc, knn_b_acc, nbayes_b_acc, rulefit_b_acc) |>
select(model, mean, std_err, n) |>
arrange(mean) |>
distinct()
table_b_accuracy
# Setup pre-processing/recipes - M1 just groups
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
load(here("attempt_2/results/data_split.rda"))
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
# M1 recipe - just groups
## for non tree (parametric) models
# Setup pre-processing/recipes - M1 just groups
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
load(here("attempt_2/results/data_split.rda"))
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
# M1 recipe - just groups
## for non tree (parametric) models
m1_recipe <- recipe(depression_ever ~ data_group, sexual_orientation,
nativity_status, region, urban_rural, race_category, age,
hispanic_ethnicity, hispanic_and_race, hispanic_details, data = data_train) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_novel(all_nominal_predictors()) |>
# Convert nominal predictors to dummy variables
step_dummy(all_nominal_predictors()) |>
# Remove zero-variance predictors
step_zv(all_predictors()) |>
# Normalize predictors
step_normalize(all_predictors())
prep(m1_recipe) |>
bake(new_data = NULL)
m1_recipe <- recipe(depression_ever ~ data_group, sexual_orientation,
nativity_status, region, urban_rural, race_category, age,
hispanic_ethnicity, hispanic_and_race, hispanic_details, data = data_train) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_novel(all_nominal_predictors()) |>
# Convert nominal predictors to dummy variables
step_dummy(all_nominal_predictors()) |>
# Remove zero-variance predictors
step_zv(all_predictors()) |>
# Normalize predictors
step_normalize(all_predictors())
prep(m1_recipe) |>
bake(new_data = NULL)
m1_recipe <- recipe(depression_ever ~ sexual_orientation,
nativity_status, region, urban_rural, race_category, age,
hispanic_ethnicity, hispanic_and_race, hispanic_details, data = data_train) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_novel(all_nominal_predictors()) |>
# Convert nominal predictors to dummy variables
step_dummy(all_nominal_predictors()) |>
# Remove zero-variance predictors
step_zv(all_predictors()) |>
# Normalize predictors
step_normalize(all_predictors())
prep(m1_recipe) |>
bake(new_data = NULL)
m1_recipe <- recipe(depression_ever ~ sexual_orientation +
nativity_status + region + urban_rural + race_category +
age + hispanic_ethnicity + hispanic_and_race + hispanic_details,
data = data_train) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_novel(all_nominal_predictors()) |>
# Convert nominal predictors to dummy variables
step_dummy(all_nominal_predictors()) |>
# Remove zero-variance predictors
step_zv(all_predictors()) |>
# Normalize predictors
step_normalize(all_predictors())
prep(m1_recipe) |>
bake(new_data = NULL)
m1_recipe <- recipe(depression_ever ~ data_group + region + urban_rural + race_category +
age + hispanic_ethnicity + hispanic_and_race + hispanic_details,
data = data_train) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_novel(all_nominal_predictors()) |>
# Convert nominal predictors to dummy variables
step_dummy(all_nominal_predictors()) |>
# Remove zero-variance predictors
step_zv(all_predictors()) |>
# Normalize predictors
step_normalize(all_predictors())
prep(m1_recipe) |>
bake(new_data = NULL)
## for  tree (non-parametric) models
m1_recipe_t <- recipe(depression_ever ~ data_group + region + urban_rural + race_category +
age + hispanic_ethnicity + hispanic_and_race + hispanic_details,
data = data_train) |>
# Remove other unnecessary variables
#step_rm(year) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_novel(all_nominal_predictors()) |>
step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
step_zv(all_predictors()) |>
step_normalize(all_predictors())
prep(m1_recipe_t) |>
bake(new_data = NULL)
ll <- prep(m1_recipe_t) |>
bake(new_data = NULL)
View(ll)
# Setup pre-processing/recipes - M1 just groups
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
load(here("attempt_2/results/data_split.rda"))
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
# M1 recipe - just groups
## for non tree (parametric) models
m1_recipe <- recipe(depression_ever ~ data_group + region + urban_rural + race_category +
age + hispanic_ethnicity + hispanic_and_race + hispanic_details,
data = data_train) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_novel(all_nominal_predictors()) |>
# Convert nominal predictors to dummy variables
step_dummy(all_nominal_predictors()) |>
# Remove zero-variance predictors
step_zv(all_predictors()) |>
# Normalize predictors
step_normalize(all_predictors())
prep(m1_recipe) |>
bake(new_data = NULL)
## for  tree (non-parametric) models
m1_recipe_t <- recipe(depression_ever ~ data_group + region + urban_rural + race_category +
age + hispanic_ethnicity + hispanic_and_race + hispanic_details,
data = data_train) |>
# Remove other unnecessary variables
#step_rm(year) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_novel(all_nominal_predictors()) |>
step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
step_zv(all_predictors()) |>
step_normalize(all_predictors())
prep(m1_recipe_t) |>
bake(new_data = NULL)
save(m1_recipe, m1_recipe_t, file = here("attempt_2/recipes/m1_recipes.rda"))
# Setup pre-processing/recipes - M1 just groups
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
load(here("attempt_2/results/data_split.rda"))
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
# M1 recipe - just groups
## for non tree (parametric) models
m1_recipe <- recipe(depression_ever ~ data_group + region + urban_rural + race_category +
age + sex + hispanic_ethnicity + hispanic_and_race + hispanic_details,
data = data_train) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_novel(all_nominal_predictors()) |>
# Convert nominal predictors to dummy variables
step_dummy(all_nominal_predictors()) |>
# Remove zero-variance predictors
step_zv(all_predictors()) |>
# Normalize predictors
step_normalize(all_predictors())
prep(m1_recipe) |>
bake(new_data = NULL)
## for  tree (non-parametric) models
m1_recipe_t <- recipe(depression_ever ~ data_group + region + urban_rural + race_category +
age + sex + hispanic_ethnicity + hispanic_and_race + hispanic_details,
data = data_train) |>
# Remove other unnecessary variables
#step_rm(year) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_novel(all_nominal_predictors()) |>
step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
step_zv(all_predictors()) |>
step_normalize(all_predictors())
prep(m1_recipe_t) |>
bake(new_data = NULL)
save(m1_recipe, m1_recipe_t, file = here("attempt_2/recipes/m1_recipes.rda"))
# Logistic Regression
# # load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
load(here("attempt_2/results/data_split.rda"))
load(here("attempt_2/recipes/m1_recipes.rda"))
library(doMC)
registerDoMC(cores = parallel::detectCores(logical =TRUE))
# model specifications ----
logistic_model <- logistic_reg() |>
set_engine("glm") |>
set_mode("classification")
# define workflows ----
logistic_workflow <- workflow() |>
add_model(logistic_model) |>
add_recipe(m1_recipe)
# fit workflows/models ----
logistic_fit_b <- fit_resamples(logistic_workflow,
resamples = data_fold,
control = control_resamples(
save_workflow = TRUE,
parallel_over = "everything"))
# when tuning we need to make a grid
# write out results (fitted/trained workflows) ----
save(logistic_fit_b, file = "attempt_2/results/b_logistic_fit.rda")
# naive bayes
# Load packages ----
library(tidyverse)
library(tidymodels)
library(here)
library(discrim)
# Handle common conflicts
tidymodels_prefer()
# Set seed
set.seed(301)
# Load preprocessed data and recipe ----
load(here("attempt_2/results/data_split.rda"))
load(here("attempt_2/recipes/m1_recipes.rda"))
# Enable parallel processing ----
library(doMC)
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# Model specifications ----
nbayes_model <-
naive_Bayes(
mode = "classification",
smoothness = tune(), # Hyperparameter for Laplace smoothing
Laplace = tune()     # Additional smoothing parameter
) |>
set_engine("klaR")      # Ensure 'klaR' package is installed
# Define workflow ----
nbayes_workflow <- workflow() |>
add_model(nbayes_model) |>
add_recipe(m1_recipe)
# Hyperparameter tuning values ----
nbayes_params <- extract_parameter_set_dials(nbayes_model)
# Create grid for tuning ----
nbayes_grid <- grid_regular(nbayes_params, levels = 3)
# Fit workflows/models ----
nbayes_fit_b <- tune_grid(
nbayes_workflow,
resamples = data_fold, # Assuming `data_fold` is your cross-validation folds
grid = nbayes_grid,
control = control_grid(save_workflow = TRUE)
)
# Load packages ----
library(tidyverse)
library(tidyverse)
library(tidymodels)
library(here)
library(xrf)
library(rules)
tidymodels_prefer()
# Set seed
set.seed(301)
# Load preprocessed data and recipe ----
load(here("attempt_2/results/data_split.rda"))
load(here("attempt_2/recipes/m1_recipes.rda"))
# Enable parallel processing ----
library(doMC)
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# Set RuleFit model specification
rulefit_model <- rule_fit(
mode = "classification",
penalty = tune()
) |>
set_engine("xrf")
rulefit_workflow <- workflow() |>
add_model(rulefit_model) |>
add_recipe(m1_recipe_t)
rulefit_params <- extract_parameter_set_dials(rulefit_model)
rulefit_grid <- grid_regular(rulefit_params, levels = 3)
rulefit_fit_b <- tune_grid(
rulefit_workflow,
resamples = data_fold, # `data_fold` is my cross-validation folds
grid = rulefit_grid,
control = control_grid(save_workflow = TRUE)
)
save(rulefit_fit_b, file = here("attempt_2/results/b_rulefit_fit.rda"))
# naive bayes
# Load packages ----
library(tidyverse)
library(tidymodels)
library(here)
library(discrim)
# Handle common conflicts
tidymodels_prefer()
# Set seed
set.seed(301)
# Load preprocessed data and recipe ----
load(here("attempt_2/results/data_split.rda"))
load(here("attempt_2/recipes/m1_recipes.rda"))
# Enable parallel processing ----
library(doMC)
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# Model specifications ----
nbayes_model <-
naive_Bayes(
mode = "classification",
smoothness = tune(), # Hyperparameter for Laplace smoothing
Laplace = tune()     # Additional smoothing parameter
) |>
set_engine("klaR")      # Ensure 'klaR' package is installed
# Define workflow ----
nbayes_workflow <- workflow() |>
add_model(nbayes_model) |>
add_recipe(m1_recipe)
# Hyperparameter tuning values ----
nbayes_params <- extract_parameter_set_dials(nbayes_model)
# Create grid for tuning ----
nbayes_grid <- grid_regular(nbayes_params, levels = 3)
# Fit workflows/models ----
nbayes_fit_b <- tune_grid(
nbayes_workflow,
resamples = data_fold, # Assuming `data_fold` is your cross-validation folds
grid = nbayes_grid,
control = control_grid(save_workflow = TRUE)
)
# Save results (fitted/trained workflows) ----
save(nbayes_fit_b, file = "attempt_2/results/b_nbayes_fit.rda")
# Analysis of trained models (comparisons)
# Select final model
# Fit & analyze final model
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
# load data, recipes and fits
load(here("attempt_2/results/data_split.rda"))
load(here("attempt_2/recipes/m1_recipes.rda"))
load(here("attempt_2/results/b_logistic_fit.rda"))
load(here("attempt_2/results/b_dtree_fit.rda"))
load(here("attempt_2/results/b_nbayes_fit.rda"))
load(here("attempt_2/results/b_knn_fit.rda"))
load(here("attempt_2/results/b_rulefit_fit.rda"))
library(doMC)
registerDoMC(cores = parallel::detectCores(logical =TRUE))
# results
logistic_result <- collect_metrics(logistic_fit_b) |>
mutate(model = "logistic")
logistic_result
dtree_result <- collect_metrics(dtree_fit_b) |>
mutate(model = "decision tree")
dtree_result
knn_result <- collect_metrics(knn_fit_b) |>
mutate(model = "KNN")
knn_result
nbayes_result <- collect_metrics(nbayes_fit_b) |>
mutate(model = "Naive Bayes")
nbayes_result
rulefit_result <- collect_metrics(rulefit_fit_b) |>
mutate(model = "RuleFit")
rulefit_result
log_b_acc <- logistic_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Logistic M1")
dtree_b_acc <- dtree_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Decision Tree M1")
knn_b_acc <- knn_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "KNN M1")
nbayes_b_acc <- nbayes_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Naive Bayes M1")
rulefit_b_acc <- rulefit_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "RuleFit M1")
table_b_accuracy <- bind_rows(log_b_acc, dtree_b_acc, knn_b_acc, nbayes_b_acc, rulefit_b_acc) |>
select(model, mean, std_err, n) |>
arrange(mean) |>
distinct()
table_b_accuracy
dtree_autoplot_b <- autoplot(dtree_fit_b, metric = "accuracy")
knn_autoplot_b <- autoplot(knn_fit_b, metric = "accuracy")
rulefit_autoplot_b <- autoplot(rulefit_fit_b, metric = "accuracy")
dtree_autoplot_b
knn_autoplot_b
rulefit_autoplot_b
save(table_b_accuracy, dtree_autoplot_b,
knn_autoplot_b, rulefit_autoplot_b,
file = here("attempt_2/results/metric_results_b.rda"))
library(tidyverse)
library(tidymodels)
library(here)
load(here("attempt_1/results/metric_results.rda"))
library(tidyverse)
library(tidymodels)
library(here)
load(here("attempt_1/results/metric_results_a.rda"))
#| echo: FALSE
library(tidyverse)
library(tidymodels)
library(here)
load(here("attempt_1/results/metric_results_a.rda"))
load(here("attempt_2/results/metric_results_b.rda"))
#| echo: FALSE
library(tidyverse)
library(tidymodels)
library(here)
load(here("attempt_1/results/metric_results_a.rda"))
View(table_b_accuracy)
View(table_a_accuracy)
#| label: tbl-metrics_ks
#| tbl-cap: "Accuracy of Kitchen Sink Recipe Based Models"
#| echo: FALSE
library(tidyverse)
library(tidymodels)
library(here)
library(knitr)
load(here("attempt_1/results/metric_results_a.rda"))
table_a_accuracy |>
kable()
#| label: tbl-metrics_m1
#| tbl-cap: "Accuracy of Feature Engineered Recipe Based Models"
#| echo: FALSE
load(here("attempt_2/results/metric_results_b.rda"))
table_b_accuracy |>
kable()
