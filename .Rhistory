install.packages("DALEXtra")
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))       # loads ks tree recipe
load(here("model_explainability/quest_files/ks_dtree-fit"))     # loads dtree fit
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))       # loads ks tree recipe
load(here("model_explainability/quest_files/ks_dtree_fit"))     # loads dtree fit
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))     #
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))       # loads ks tree recipe
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))     # loads dtree fit
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))       # loads ks tree recipe
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))     # loads dtree fit
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))       # loads data split train
load(here("model_explainability/quest_files/ks_recipes.rda")) # loads ks tree recipe
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))     # loads dtree fit
# Apply recipe to training data
# Apply recipe to training data
baked_train <- bake(ks_recipe_t, new_data = NULL)
ks_recipe_t <- prep(ks_recipe_t)
baked_train <- bake(ks_recipe_t, new_data = NULL)
# Build the Explainer
explainer_ks_tree <- explain_tidymodels(
model = dtree_fit,
data = baked_train |> select(-depression),  # predictors only
y = baked_train$depression,                  # outcome variable
label = "Quest – Kitchen Sink – Decision Tree",
verbose = FALSE
)
dtree_fit
# Apply recipe to training data
ks_recipe_t <- prep(ks_recipe_t)
baked_train <- bake(ks_recipe_t, new_data = NULL)
# Add depression column from training split
baked_train$depression <- data_train$depression
# Build the Explainer
explainer_ks_tree <- explain_tidymodels(
model = dtree_fit,
data = baked_train |> select(-depression),  # predictors only
y = baked_train$depression,                  # outcome variable
label = "Quest – Kitchen Sink – Decision Tree",
verbose = FALSE
)
ks_recipe_t <- prep(ks_recipe_t)
baked_train <- baked_train |>
mutate(depression = data_train$depression)
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))       # loads data split train
load(here("model_explainability/quest_files/ks_recipes.rda")) # loads ks tree recipe
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))     # loads dtree fit
# Apply recipe to training data
ks_recipe_t <- prep(ks_recipe_t)
baked_train <- bake(ks_recipe_t, new_data = NULL)
# Add depression column from training split
baked_train$depression <- data_train$depression_ever
# Build the Explainer
explainer_ks_tree <- explain_tidymodels(
model = dtree_fit,
data = baked_train |> select(-depression_ever),  # predictors only
y = baked_train$depression_ever,                  # outcome variable
label = "Quest – Kitchen Sink – Decision Tree",
verbose = FALSE
)
explainer_ks_tree
vip_ks_tree <- model_parts(explainer_ks_tree, type = "difference")
# ---- Due to tuning
# 1. Select best parameters by F1
best_params <- select_best(dtree_fit, metric = "f_meas")
# 2. Finalize your workflow (replace this with your actual workflow object)
final_wf <- finalize_workflow(dtree_wf, best_params)
# ---- Due to tuning
dtree_wf <- extract_workflow(dtree_fit)
# 1. Select best parameters by F1
best_params <- select_best(dtree_fit, metric = "f_meas")
# 1. Get best hyperparameters
best_params <- select_best(dtree_fit, metric = "f_meas")
# 2. Finalize the workflow
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
# 3. Fit it to your full training data
final_dtree_fit <- fit(final_dtree_wf, data = baked_train)
# 3. Fit it to your full training data
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
final_dtree_fit
# Bake it again manually for explainability
ks_recipe_t <- prep(ks_recipe_t)
baked_train <- bake(prep(ks_recipe_t), new_data = NULL)
baked_train$depression_ever <- data_train$depression_ever
# Build explainer
explainer_ks_tree <- explain_tidymodels(
model = final_dtree_fit,
data = baked_train |> select(-depression_ever),
y = baked_train$depression_ever,
label = "Quest – Kitchen Sink – Decision Tree",
verbose = FALSE
)
vip_ks_tree <- model_parts(explainer_ks_tree, type = "difference")
# Plot
plot(vip_ks_tree)
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))       # data_split
load(here("model_explainability/quest_files/ks_recipes.rda"))          # ks_recipe_t
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))        # dtree_fit (tuning result)
# ---- Finalize & Fit the Model ----
dtree_wf <- extract_workflow(dtree_fit)
# 1. Get best hyperparameters
best_params <- select_best(dtree_fit, metric = "f_meas")
# 2. Finalize the workflow
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
# 3. Get original training data
data_train <- training(data_split)
# 4. Fit final model to full training data
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
# ---- Prepare Data for Explainability ----
ks_recipe_t <- prep(ks_recipe_t)
baked_train <- bake(ks_recipe_t, new_data = NULL)
baked_train$depression_ever <- data_train$depression_ever
# ---- Build the Explainer ----
explainer_ks_tree <- explain_tidymodels(
model = final_dtree_fit,
data = baked_train |> select(-depression_ever),
y = baked_train$depression_ever,
label = "Quest – Kitchen Sink – Decision Tree",
verbose = FALSE
)
# ---- Variable Importance (Global) ----
vip_ks_tree <- model_parts(explainer_ks_tree, type = "difference")
# ---- Build the Explainer ----
explainer_ks_tree <- explain_tidymodels(
model = final_dtree_fit,
data = data_train |> select(-depression_ever),
y = baked_train$depression_ever,
label = "Quest – Kitchen Sink – Decision Tree",
verbose = FALSE
)
# ---- Variable Importance (Global) ----
vip_ks_tree <- model_parts(explainer_ks_tree, type = "difference")
# ---- Variable Importance (Global) ----
vip_ks_tree <- model_parts(explainer_ks_tree, type = "difference")
# ---- Variable Importance (Global) ----
vip_ks_tree <- model_parts(explainer_ks_tree, type = "raw")
library(vip)
# ---- Variable Importance (Global) ----
vip_ks_tree <- model_parts(explainer_ks_tree, type = "difference")
# ---- Variable Importance (Global) ----
vip_ks_tree <- model_parts(explainer_ks_tree, type = "raw")
# Extract the fitted engine (actual decision tree object)
tree_model <- extract_fit_parsnip(final_dtree_fit)$fit
# ---- Variable Importance (Global) ----
vip(tree_model, num_features = 15,
bar = TRUE,
aesthetics = list(fill = "steelblue"),
title = "Variable Importance – Decision Tree (Quest – Kitchen Sink)")
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(vip)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))   # data_split
load(here("model_explainability/quest_files/ks_recipes.rda"))      # ks_recipe_t
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))    # dtree_fit (tuning result)
# ---- Finalize & Fit the Model ----
dtree_wf <- extract_workflow(dtree_fit)
# 1. Get best hyperparameters
best_params <- select_best(dtree_fit, metric = "f_meas")
# 2. Finalize the workflow
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
# 3. Get original training data
data_train <- training(data_split)
# 4. Fit final model to full training data
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
# ---- Extract fitted tree model for explainability ----
tree_model <- extract_fit_parsnip(final_dtree_fit)$fit
# ---- Create VIP Plot ----
vip_plot <- vip(tree_model,
num_features = 15,
bar = TRUE,
aesthetics = list(fill = "steelblue"),
title = "Variable Importance – Decision Tree (Quest – Kitchen Sink)")
# ---- Save Plot ----
ggsave(
filename = here("model_explainability/results/vip_dtree_quest_ks.png"),
plot = vip_plot,
width = 10, height = 6,
dpi = 300
)
save(vip_plot, here("model_explainability/results/vip_dtree_quest_ks.rda"))
save(vip_plot, file = here("model_explainability/results/vip_dtree_quest_ks.rda"))
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(vip)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/m3_data_split.rda"))   # data_split
load(here("model_explainability/quest_files/m3_recipes.rda"))      # m3_recipe_t
load(here("model_explainability/quest_files/3c_dtree_fit.rda"))
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/m3_data_split.rda"))   # data_split
load(here("model_explainability/quest_files/m3_recipes.rda"))      # m3_recipe_t
load(here("model_explainability/quest_files/m3_dtree_fit.rda"))    #
# ---- Finalize & Fit the Model ----
dtree_wf <- extract_workflow(dtree_fit_c)
# 1. Get best hyperparameters
best_params <- select_best(dtree_fit_c, metric = "f_meas")
# 2. Finalize the workflow
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
# 3. Get original training data
data_train <- training(data_split)
# 4. Fit final model to full training data
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
# ---- Extract fitted tree model for explainability ----
tree_model <- extract_fit_parsnip(final_dtree_fit)$fit
# ---- Create VIP Plot ----
vip_plot <- vip(tree_model,
num_features = 15,
bar = TRUE,
aesthetics = list(fill = "steelblue"),
title = "Variable Importance – Decision Tree (Quest – M3)")
# ---- Create VIP Plot ----
vip_plot_m3 <- vip(tree_model,
num_features = 15,
bar = TRUE,
aesthetics = list(fill = "steelblue"),
title = "Variable Importance – Decision Tree (Quest – M3)")
vip_plot_m3
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(vip)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/m3_data_split.rda"))   # data_split
load(here("model_explainability/quest_files/m3_recipes.rda"))      # m3_recipe_t
load(here("model_explainability/quest_files/m3_dtree_fit.rda"))    # dtree_fit (tuning result)
# ---- Finalize & Fit the Model ----
dtree_wf <- extract_workflow(dtree_fit_c)
# 1. Get best hyperparameters
best_params <- select_best(dtree_fit_c, metric = "f_meas")
# 2. Finalize the workflow
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
# 3. Get original training data
data_train <- training(data_split)
# 4. Fit final model to full training data
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
# ---- Extract fitted tree model for explainability ----
tree_model <- extract_fit_parsnip(final_dtree_fit)$fit
# ---- Create VIP Plot ----
vip_plot_m3 <- vip(tree_model,
num_features = 15,
bar = TRUE,
aesthetics = list(fill = "steelblue"),
title = "Variable Importance – Decision Tree (Quest – M3)")
# ---- Save Plot ----
ggsave(
filename = here("model_explainability/results/vip_dtree_quest_m3.png"),
plot = vip_plot_m3,
width = 10, height = 6,
dpi = 300
)
save(vip_plot_m3, file = here("model_explainability/results/vip_dtree_quest_m3.rda"))
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(vip)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))   # data_split
load(here("model_explainability/quest_files/ks_recipes.rda"))      # ks_recipe_t
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))    # dtree_fit (tuning result)
# ---- Finalize & Fit the Model ----
dtree_wf <- extract_workflow(dtree_fit)
# 1. Get best hyperparameters
best_params <- select_best(dtree_fit, metric = "f_meas")
# 2. Finalize the workflow
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
# 3. Get original training data
data_train <- training(data_split)
# 4. Fit final model to full training data
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
# ---- Extract fitted tree model for explainability ----
tree_model <- extract_fit_parsnip(final_dtree_fit)$fit
# ---- Create VIP Plot ----
vip_plot_ks <- vip(tree_model,
num_features = 15,
bar = TRUE,
aesthetics = list(fill = "steelblue"),
title = "Variable Importance – Decision Tree (Quest – Kitchen Sink)")
# ---- Save Plot ----
ggsave(
filename = here("model_explainability/results/vip_dtree_quest_ks.png"),
plot = vip_plot_ks,
width = 10, height = 6,
dpi = 300
)
save(vip_plot_ks, file = here("model_explainability/results/vip_dtree_quest_ks.rda"))
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
library(ingredients)
# ---- Load Model & Data ----
load(here("model_explainability/quest_files/ks_data_split.rda"))   # data_split
load(here("model_explainability/quest_files/ks_recipes.rda"))      # ks_recipe_t
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))    # dtree_fit
# ---- Finalize & Fit Model ----
dtree_wf <- extract_workflow(dtree_fit)
best_params <- select_best(dtree_fit, metric = "f_meas")
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
# ---- Save Plot ----
ggsave(
filename = here("model_explainability/results/shap_explanation_ks_tree.png"),
plot = shap_plot,
width = 10, height = 6,
dpi = 300
)
# ---- Save Object (optional) ----
save(shap_plot, shap_ks_tree, file = here("model_explainability/results/shap_explanation_ks_tree.rda"))
shap_plot
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
library(ingredients)
# ---- Load Model & Data ----
load(here("model_explainability/quest_files/m3_data_split.rda"))   # data_split
load(here("model_explainability/quest_files/m3_recipes.rda"))      # m3_recipe_t
load(here("model_explainability/quest_files/m3_dtree_fit.rda"))    # dtree_fit
# ---- Finalize & Fit Model ----
dtree_wf <- extract_workflow(dtree_fit)
best_params <- select_best(dtree_fit, metric = "f_meas")
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
# ---- Build Explainer ----
explainer_m3_tree <- explain_tidymodels(
model = final_dtree_fit,
data = data_train |> select(-depression_ever),
y = data_train$depression_ever,
label = "Quest – M3 – Decision Tree",
verbose = FALSE
)
# ---- Build Explainer ----
explainer_m3_tree <- explain_tidymodels(
model = final_dtree_fit,
data = data_train |> select(-depression_ever),
y = data_train$depression_ever,
label = "Quest – M3 – Decision Tree",
verbose = FALSE
)
# Option 2 (optional): A specific case (e.g., LGBTQ+ migrant)
example_obs <- data_train %>% filter(data_group == "LGBTQ_Migrant") %>% slice(1)
# ---- Compute SHAP Values ----
shap_m3_tree <- predict_parts(
explainer = explainer_m3_tree,
new_observation = example_obs,
type = "shap"
)
shap_plot_m3
# ---- Plot SHAP ----
shap_plot_m3 <- plot(shap_m3_tree) +
labs(
title = "SHAP Explanation – Single Prediction",
subtitle = "Quest – M3 – Decision Tree"
)
shap_plot_m3
# ---- Save Plot ----
ggsave(
filename = here("model_explainability/results/shap_explanation_m3_tree.png"),
plot = shap_plot,
width = 10, height = 6,
dpi = 300
)
# ---- Save Object (optional) ----
save(shap_plot_m3, shap_m3_tree, file = here("model_explainability/results/shap_explanation_m3_tree.rda"))
gc()
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
# ---- Load Final Model & Training Data ----
load(here("model_explainability/quest_files/ks_data_split.rda"))     # data_split
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))      # dtree_fit
load(here("model_explainability/quest_files/ks_recipes.rda"))        # ks_recipe_t
# ---- Finalize & Fit the Model ----
dtree_wf <- extract_workflow(dtree_fit)
best_params <- select_best(dtree_fit, metric = "f_meas")
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
# ---- Build Explainer for Local Explanation ----
explainer_ks_tree <- explain_tidymodels(
model = final_dtree_fit,
data = data_train |> select(-depression_ever),
y = data_train$depression_ever,
label = "Quest – Kitchen Sink – Decision Tree",
verbose = FALSE
)
# ---- Choose One Observation to Explain ----
one_person <- data_train |>
filter(data_group == "LGBTQ_Migrant") |>
slice_sample(n = 1)
# ---- Local Explanation with predict_parts() ----
local_explanation_ks <- predict_parts(
explainer = explainer_ks_tree,
new_observation = one_person,
type = "break_down"
)
# ---- Plot Local Explanation ----
local_plot_ks <- plot(local_explanation_ks) +
labs(title = "Local Explanation – Quest (Kitchen Sink – Decision Tree)")
# ---- Save Plot ----
ggsave(
filename = here("model_explainability/results/local_explanation_ks.png"),
plot = local_plot_ks,
width = 10, height = 6,
dpi = 300
)
save(local_explanation_ks, local_plot_ks, file = here("model_explainability/results/local_explanation_ks.rda"))
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
library(ingredients)
# ---- Load Model & Data ----
load(here("model_explainability/quest_files/m3_data_split.rda"))   # data_split
load(here("model_explainability/quest_files/m3_recipes.rda"))      # m3_recipe_t
load(here("model_explainability/quest_files/m3_dtree_fit.rda"))    # dtree_fit
# ---- Finalize & Fit Model ----
dtree_wf <- extract_workflow(dtree_fit)
best_params <- select_best(dtree_fit, metric = "f_meas")
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
data_train <- training(data_split)
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
# ---- Build Explainer ----
explainer_m3_tree <- explain_tidymodels(
model = final_dtree_fit,
data = data_train |> select(-depression_ever),
y = data_train$depression_ever,
label = "Quest – M3 – Decision Tree",
verbose = FALSE
)
# ---- Select Individual for SHAP ----
example_obs <- data_train %>%
filter(data_group == "LGBTQ_Migrant") %>%
slice(1)
# Optional: print the observation to document who it is
print(example_obs)
# ---- Compute SHAP Values ----
shap_m3_tree <- predict_parts(
explainer = explainer_m3_tree,
new_observation = example_obs,
type = "shap"
)
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
library(ingredients)
# ---- Load Model & Data ----
load(here("model_explainability/quest_files/m3_data_split.rda"))   # data_split
load(here("model_explainability/quest_files/m3_recipes.rda"))      # m3_recipe_t
load(here("model_explainability/quest_files/m3_dtree_fit.rda"))    # dtree_fit
gc()
gc()
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
library(ingredients)
# ---- Load Model & Data ----
load(here("model_explainability/quest_files/m3_data_split.rda"))   # data_split
load(here("model_explainability/quest_files/m3_recipes.rda"))      # m3_recipe_t
load(here("model_explainability/quest_files/m3_dtree_fit.rda"))    # dtree_fit
# ---- Finalize & Fit Model ----
dtree_wf <- extract_workflow(dtree_fit)
# ---- Finalize & Fit Model ----
dtree_wf <- extract_workflow(dtree_fit_c)
best_params <- select_best(dtree_fit, metric = "f_meas")
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
