save(m3_recipe, m3_recipe_t, file = here("attempt_3/recipes/m3_recipes.rda"))
# Logistic Regression
# # load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
load(here("attempt_3/results/data_split.rda"))
load(here("attempt_3/recipes/m3_recipes.rda"))
library(doMC)
registerDoMC(cores = parallel::detectCores(logical =TRUE))
# model specifications ----
logistic_model <- logistic_reg() |>
set_engine("glm") |>
set_mode("classification")
# define workflows ----
logistic_workflow <- workflow() |>
add_model(logistic_model) |>
add_recipe(m3_recipe)
# fit workflows/models ----
logistic_fit_c <- fit_resamples(logistic_workflow,
resamples = data_fold,
control = control_resamples(
save_workflow = TRUE,
parallel_over = "everything"))
# when tuning we need to make a grid
# write out results (fitted/trained workflows) ----
save(logistic_fit_c, file = "attempt_3/results/c_logistic_fit.rda")
gc()
gc()
# Analysis of trained models (comparisons)
# Select final model
# Fit & analyze final model
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
# load data, recipes and fits
load(here("attempt_3/results/data_split.rda"))
load(here("attempt_3/recipes/m3_recipes.rda"))
load(here("attempt_3/results/c_logistic_fit.rda"))
load(here("attempt_3/results/c_dtree_fit.rda"))
load(here("attempt_3/results/c_nbayes_fit.rda"))
load(here("attempt_3/results/c_knn_fit.rda"))
load(here("attempt_3/results/c_rulefit_fit.rda"))
library(doMC)
registerDoMC(cores = parallel::detectCores(logical =TRUE))
# results
logistic_result <- collect_metrics(logistic_fit_c) |>
mutate(model = "logistic")
logistic_result
dtree_result <- collect_metrics(dtree_fit_c) |>
mutate(model = "decision tree")
dtree_result
knn_result <- collect_metrics(knn_fit_c) |>
mutate(model = "KNN")
knn_result
nbayes_result <- collect_metrics(nbayes_fit_c) |>
mutate(model = "Naive Bayes")
nbayes_result
rulefit_result <- collect_metrics(rulefit_fit_c) |>
mutate(model = "RuleFit")
rulefit_result
log_c_acc <- logistic_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Logistic M3")
dtree_c_acc <- dtree_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Decision Tree M3")
knn_c_acc <- knn_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "KNN M3")
nbayes_c_acc <- nbayes_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Naive Bayes M3")
rulefit_c_acc <- rulefit_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "RuleFit M3")
table_c_accuracy <- bind_rows(log_c_acc, dtree_c_acc, knn_c_acc, nbayes_c_acc, rulefit_c_acc) |>
select(model, mean, std_err, n) |>
arrange(mean) |>
distinct()
table_c_accuracy
dtree_autoplot_c <- autoplot(dtree_fit_c, metric = "accuracy")
knn_autoplot_c <- autoplot(knn_fit_c, metric = "accuracy")
rulefit_autoplot_c <- autoplot(rulefit_fit_c, metric = "accuracy")
# nbayes_autoplot_a <- autoplot(nbayes_fit, metric = "accuracy")
save(table_c_accuracy, dtree_autoplot_c,
knn_autoplot_c, rulefit_autoplot_c,
file = here("attempt_3/results/metric_results_c.rda"))
# Analysis of trained models (comparisons)
# Select final model
# Fit & analyze final model
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
# load data, recipes and fits
load(here("attempt_2/results/data_split.rda"))
load(here("attempt_2/recipes/m2_recipes.rda"))
load(here("attempt_2/results/b_logistic_fit.rda"))
load(here("attempt_2/results/b_dtree_fit.rda"))
load(here("attempt_2/results/b_nbayes_fit.rda"))
load(here("attempt_2/results/b_knn_fit.rda"))
load(here("attempt_2/results/b_rulefit_fit.rda"))
library(doMC)
registerDoMC(cores = parallel::detectCores(logical =TRUE))
# results
logistic_result <- collect_metrics(logistic_fit_b) |>
mutate(model = "logistic")
logistic_result
dtree_result <- collect_metrics(dtree_fit_b) |>
mutate(model = "decision tree")
dtree_result
knn_result <- collect_metrics(knn_fit_b) |>
mutate(model = "KNN")
knn_result
nbayes_result <- collect_metrics(nbayes_fit_b) |>
mutate(model = "Naive Bayes")
nbayes_result
rulefit_result <- collect_metrics(rulefit_fit_b) |>
mutate(model = "RuleFit")
rulefit_result
log_b_acc <- logistic_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Logistic M2")
dtree_b_acc <- dtree_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Decision Tree M2")
knn_b_acc <- knn_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "KNN M2")
nbayes_b_acc <- nbayes_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Naive Bayes M2")
rulefit_b_acc <- rulefit_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "RuleFit M2")
table_b_accuracy <- bind_rows(log_b_acc, dtree_b_acc, knn_b_acc, nbayes_b_acc, rulefit_b_acc) |>
select(model, mean, std_err, n) |>
arrange(mean) |>
distinct()
table_b_accuracy
dtree_autoplot_b <- autoplot(dtree_fit_b, metric = "accuracy")
knn_autoplot_b <- autoplot(knn_fit_b, metric = "accuracy")
rulefit_autoplot_b <- autoplot(rulefit_fit_b, metric = "accuracy")
# nbayes_autoplot_a <- autoplot(nbayes_fit, metric = "accuracy")
save(table_b_accuracy, dtree_autoplot_b,
knn_autoplot_b, rulefit_autoplot_b,
file = here("attempt_2/results/metric_results_b.rda"))
table_b_accuracy
table_c_accuracy
# Analysis of trained models (comparisons)
# Select final model
# Fit & analyze final model
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
# load data, recipes and fits
load(here("attempt_1/results/data_split.rda"))
load(here("attempt_1/recipes/ks_recipes.rda"))
load(here("attempt_1/results/a_logistic_fit.rda"))
load(here("attempt_1/results/a_dtree_fit.rda"))
load(here("attempt_1/results/a_nbayes_fit.rda"))
load(here("attempt_1/results/a_knn_fit.rda"))
load(here("attempt_1/results/a_rulefit_fit.rda"))
library(doMC)
registerDoMC(cores = parallel::detectCores(logical =TRUE))
# results
logistic_result <- collect_metrics(logistic_fit) |>
mutate(model = "logistic")
logistic_result
dtree_result <- collect_metrics(dtree_fit) |>
mutate(model = "decision tree")
dtree_result
knn_result <- collect_metrics(knn_fit) |>
mutate(model = "KNN")
knn_result
nbayes_result <- collect_metrics(nbayes_fit) |>
mutate(model = "Naive Bayes")
nbayes_result
rulefit_result <- collect_metrics(rulefit_fit) |>
mutate(model = "RuleFit")
rulefit_result
log_a_acc <- logistic_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Logistic KS")
dtree_a_acc <- dtree_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Decision Tree KS")
knn_a_acc <- knn_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "KNN KS")
nbayes_a_acc <- nbayes_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Naive Bayes KS")
rulefit_a_acc <- rulefit_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "RuleFit KS")
table_a_accuracy <- bind_rows(log_a_acc, dtree_a_acc, knn_a_acc, nbayes_a_acc, rulefit_a_acc) |>
select(model, mean, std_err, n) |>
arrange(mean) |>
distinct()
table_a_accuracy
dtree_autoplot_a <- autoplot(dtree_fit, metric = "accuracy")
knn_autoplot_a <- autoplot(knn_fit, metric = "accuracy")
rulefit_autoplot_a <- autoplot(rulefit_fit, metric = "accuracy")
# nbayes_autoplot_a <- autoplot(nbayes_fit, metric = "accuracy")
save(table_a_accuracy, dtree_autoplot_a,
knn_autoplot_a, rulefit_autoplot_a,
file = here("attempt_1/results/metric_results_a.rda"))
table_a_accuracy
table_b_accuracy
table_c_accuracy
# data splitting
# load packages ----
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
library(janitor)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
combined_data <- read_csv(here("data_combined/combined_nhis_data.csv"))
# data split
data_split <- initial_split(combined_data,
prop = 0.80,
strat = depression_ever,
breaks = 4)
data_train <- data_split |> training()
data_test <- data_split |> testing()
data_fold <- vfold_cv(data_train, v = 4, repeats = 3,
strata = depression_ever)
save(data_split, data_train, data_test, data_fold, file = here("attempt_4/results/data_split.rda"))
# Setup pre-processing/recipes - M1 just groups
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
load(here("attempt_4/results/data_split.rda"))
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
# M1 recipe
## for non tree (parametric) models
m1_recipe <- recipe(depression_ever ~ data_group + sex,
data = data_train) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_novel(all_nominal_predictors()) |>
# Convert nominal predictors to dummy variables
step_dummy(all_nominal_predictors()) |>
# Remove zero-variance predictors
step_zv(all_predictors()) |>
# Normalize predictors
step_normalize(all_predictors())
prep(m1_recipe) |>
bake(new_data = NULL)
## for  tree (non-parametric) models
m1_recipe_t <- recipe(depression_ever ~ data_group + sex,
data = data_train) |>
# Remove other unnecessary variables
#step_rm(year) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_novel(all_nominal_predictors()) |>
step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
step_zv(all_predictors()) |>
step_normalize(all_predictors())
prep(m1_recipe_t) |>
bake(new_data = NULL)
save(m1_recipe, m1_recipe_t, file = here("attempt_4/recipes/m1_recipes.rda"))
# decision tree
# Load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# Handle common conflicts
tidymodels_prefer()
# Set seed
set.seed(301)
# Load preprocessed data and recipe ----
load(here("attempt_4/results/data_split.rda"))
load(here("attempt_4/recipes/m1_recipes.rda"))
# Enable parallel processing ----
library(doMC)
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# Model specifications ----
dtree_model <-
decision_tree(
mode = "classification",
cost_complexity = tune(),   # Controls pruning
tree_depth = tune(),        # Max depth of tree
min_n = tune()              # Minimum samples required for a split
) |>
set_engine("rpart")           # Use the rpart engine
# Define workflow ----
dtree_workflow <- workflow() |>
add_model(dtree_model) |>
add_recipe(m1_recipe_t)
dtree_params <- extract_parameter_set_dials(dtree_model)
dtree_grid <- grid_regular(dtree_params, levels = 5)
dtree_fit_d <- tune_grid(
dtree_workflow,
resamples = data_fold,        # Assuming `data_fold` is your cross-validation folds
grid = dtree_grid,
control = control_grid(save_workflow = TRUE)
)
# Logistic Regression
# # load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
load(here("attempt_4/results/data_split.rda"))
load(here("attempt_4/recipes/m1_recipes.rda"))
library(doMC)
registerDoMC(cores = parallel::detectCores(logical =TRUE))
# model specifications ----
logistic_model <- logistic_reg() |>
set_engine("glm") |>
set_mode("classification")
# define workflows ----
logistic_workflow <- workflow() |>
add_model(logistic_model) |>
add_recipe(m1_recipe)
# fit workflows/models ----
logistic_fit_d <- fit_resamples(logistic_workflow,
resamples = data_fold,
control = control_resamples(
save_workflow = TRUE,
parallel_over = "everything"))
# when tuning we need to make a grid
# write out results (fitted/trained workflows) ----
save(logistic_fit_d, file = "attempt_4/results/d_logistic_fit.rda")
# RuleFit
# Load packages ----
library(tidyverse)
library(tidymodels)
library(here)
library(xrf)
library(rules)
# Handle common conflicts
tidymodels_prefer()
# Set seed
set.seed(301)
# Load preprocessed data and recipe ----
load(here("attempt_4/results/data_split.rda"))
load(here("attempt_4/recipes/m1_recipes.rda"))
# Enable parallel processing ----
library(doMC)
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# Set RuleFit model specification
rulefit_model <- rule_fit(
mode = "classification",
penalty = tune()
) |>
set_engine("xrf")
rulefit_workflow <- workflow() |>
add_model(rulefit_model) |>
add_recipe(m1_recipe_t)
rulefit_params <- extract_parameter_set_dials(rulefit_model)
rulefit_grid <- grid_regular(rulefit_params, levels = 5)
rulefit_fit_d <- tune_grid(
rulefit_workflow,
resamples = data_fold, # `data_fold` is my cross-validation folds
grid = rulefit_grid,
control = control_grid(save_workflow = TRUE)
)
save(rulefit_fit_d, file = here("attempt_4/results/d_rulefit_fit.rda"))
# Analysis of trained models (comparisons)
# Select final model
# Fit & analyze final model
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
# load data, recipes and fits
load(here("attempt_4/results/data_split.rda"))
load(here("attempt_4/recipes/m1_recipes.rda"))
load(here("attempt_4/results/c_logistic_fit.rda"))
# Analysis of trained models (comparisons)
# Select final model
# Fit & analyze final model
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
# load data, recipes and fits
load(here("attempt_4/results/data_split.rda"))
load(here("attempt_4/recipes/m1_recipes.rda"))
load(here("attempt_4/results/d_logistic_fit.rda"))
load(here("attempt_4/results/d_dtree_fit.rda"))
load(here("attempt_4/results/d_nbayes_fit.rda"))
load(here("attempt_4/results/d_knn_fit.rda"))
load(here("attempt_4/results/d_rulefit_fit.rda"))
library(doMC)
registerDoMC(cores = parallel::detectCores(logical =TRUE))
logistic_result <- collect_metrics(logistic_fit_d) |>
mutate(model = "logistic")
logistic_result
dtree_result <- collect_metrics(dtree_fit_d) |>
mutate(model = "decision tree")
dtree_result
knn_result <- collect_metrics(knn_fit_d) |>
mutate(model = "KNN")
knn_result
nbayes_result <- collect_metrics(nbayes_fit_d) |>
mutate(model = "Naive Bayes")
nbayes_result
rulefit_result <- collect_metrics(rulefit_fit_d) |>
mutate(model = "RuleFit")
rulefit_result
log_d_acc <- logistic_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Logistic M1")
dtree_d_acc <- dtree_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Decision Tree M1")
knn_d_acc <- knn_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "KNN M1")
nbayes_d_acc <- nbayes_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "Naive Bayes M1")
rulefit_d_acc <- rulefit_result |>
filter(.metric == "accuracy") |>
slice_max(mean) |>
select(mean, n, std_err) |>
mutate(model = "RuleFit M1")
table_d_accuracy <- bind_rows(log_d_acc, dtree_d_acc, knn_d_acc, nbayes_d_acc, rulefit_d_acc) |>
select(model, mean, std_err, n) |>
arrange(mean) |>
distinct()
table_d_accuracy
dtree_autoplot_d <- autoplot(dtree_fit_d, metric = "accuracy")
knn_autoplot_d <- autoplot(knn_fit_d, metric = "accuracy")
rulefit_autoplot_d <- autoplot(rulefit_fit_d, metric = "accuracy")
rulefit_autoplot_d
knn_autoplot_d
dtree_autoplot_d
save(table_d_accuracy, dtree_autoplot_d,
knn_autoplot_d, rulefit_autoplot_d,
file = here("attempt_4/results/metric_results_d.rda"))
