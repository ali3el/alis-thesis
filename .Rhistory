# Fit & analyze final model
# === Load Packages ===
library(tidyverse)
library(tidymodels)
library(here)
library(doMC)
tidymodels_prefer()
registerDoMC(cores = parallel::detectCores(logical = TRUE))
set.seed(301)
load(here("attempt_1/results/data_split.rda"))
load(here("attempt_1/recipes/ks_recipes.rda"))
load(here("attempt_1/results/a_logistic_fit.rda"))
load(here("attempt_1/results/a_dtree_fit.rda"))
load(here("attempt_1/results/a_nbayes_fit.rda"))
load(here("attempt_1/results/a_knn_fit.rda"))
load(here("attempt_1/results/a_rulefit_fit.rda"))
logistic_result <- collect_metrics(logistic_fit) |> mutate(model = "Logistic")
dtree_result    <- collect_metrics(dtree_fit)    |> mutate(model = "Decision Tree")
knn_result      <- collect_metrics(knn_fit)      |> mutate(model = "KNN")
nbayes_result   <- collect_metrics(nbayes_fit)   |> mutate(model = "Naive Bayes")
rulefit_result  <- collect_metrics(rulefit_fit)  |> mutate(model = "RuleFit")
# === Combine All Metrics ===
all_results <- bind_rows(
logistic_result,
dtree_result,
knn_result,
nbayes_result,
rulefit_result
)
# === Select Key Metrics ===
key_metrics <- c("accuracy", "precision", "recall", "f_meas", "roc_auc", "mn_log_loss")
summary_table <- all_results |>
filter(.metric %in% key_metrics) |>
group_by(model, .metric) |>
slice_max(mean, with_ties = FALSE) |>
ungroup() |>
select(model, .metric, mean, std_err, n)
# === Pivot for Clean Table Format (optional for output) ===
summary_wide <- summary_table |>
pivot_wider(
names_from = .metric,
values_from = mean
) |>
arrange(desc(f_meas))
summary_wide
# Step 1: Bind all model results into one tibble
all_results <- bind_rows(
logistic_result,
dtree_result,
knn_result,
nbayes_result,
rulefit_result
)
# Step 2: Keep only relevant metrics
metrics_to_keep <- c("accuracy", "f_meas", "precision", "recall", "roc_auc", "mn_log_loss")
summary_metrics <- all_results %>%
filter(.metric %in% metrics_to_keep) %>%
group_by(model, .metric) %>%
slice_max(mean, n = 1, with_ties = FALSE) %>%
ungroup()
# Step 3: Pivot to wide format
summary_wide <- summary_metrics %>%
select(model, .metric, mean, std_err) %>%
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
) %>%
arrange(desc(f_meas_mean))  # You can sort by f1 or any metric
# Step 4: Rename columns for clarity (optional)
summary_final <- summary_wide %>%
rename_with(~ str_replace_all(., "_mean", ""), ends_with("_mean")) %>%
rename_with(~ paste0("std_err_", .), ends_with("_std_err"))
# Step 5: View it
print(summary_final)
view(summary_final)
# =============================
# Analysis: Attempt 1 Summary Table
# =============================
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(doMC)
library(pROC)  # For AUC if used in raw ROC curves
tidymodels_prefer()
set.seed(301)
# ---- Register Parallel Processing ----
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# ---- Load Data and Fits ----
load(here("attempt_1/results/data_split.rda"))
load(here("attempt_1/recipes/ks_recipes.rda"))
load(here("attempt_1/results/a_logistic_fit.rda"))
load(here("attempt_1/results/a_dtree_fit.rda"))
load(here("attempt_1/results/a_nbayes_fit.rda"))
load(here("attempt_1/results/a_knn_fit.rda"))
load(here("attempt_1/results/a_rulefit_fit.rda"))
# ---- Combine and Filter Metrics ----
all_results <- bind_rows(
logistic_result <- collect_metrics(logistic_fit) %>% mutate(model = "Logistic"),
dtree_result    <- collect_metrics(dtree_fit)    %>% mutate(model = "Decision Tree"),
knn_result      <- collect_metrics(knn_fit)      %>% mutate(model = "KNN"),
nbayes_result   <- collect_metrics(nbayes_fit)   %>% mutate(model = "Naive Bayes"),
rulefit_result  <- collect_metrics(rulefit_fit)  %>% mutate(model = "RuleFit")
)
# ---- Define Metrics to Keep ----
metrics_to_keep <- c("accuracy", "precision", "recall", "f_meas", "roc_auc")
summary_metrics <- all_results %>%
filter(.metric %in% metrics_to_keep) %>%
group_by(model, .metric) %>%
slice_max(mean, with_ties = FALSE) %>%
ungroup()
# ---- Pivot to Wide Format and Clean ----
summary_wide <- summary_metrics %>%
select(model, .metric, mean, std_err) %>%
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
)
summary_clean <- summary_wide %>%
rename_with(~ str_replace(., "_mean", ""), ends_with("_mean")) %>%
rename_with(~ paste0("std_err_", .), ends_with("_std_err")) %>%
select(model, accuracy, precision, recall, f_meas, roc_auc, std_err_f_meas)
summary_metrics <- all_results %>%
filter(.metric %in% metrics_to_keep) %>%
group_by(model, .metric) %>%
slice_max(mean, with_ties = FALSE) %>%
ungroup()
summary_metrics
summary_metrics <- all_results %>%
filter(.metric %in% metrics_to_keep) %>%
group_by(model, .metric) %>%
slice_max(mean, with_ties = FALSE) %>%
ungroup()
summary_metrics <- all_results %>%
filter(.metric %in% metrics_to_keep) %>%
group_by(model, .metric) %>%
slice_max(mean, with_ties = FALSE) %>%
ungroup()
summary_metrics
# Step 1: Get best config per model based on F1-score
best_f1_config <- all_results %>%
filter(.metric == "f_meas") %>%
group_by(model) %>%
slice_max(mean, with_ties = FALSE) %>%
select(model, .config)
# Step 2: Join back all metrics from the best config
summary_metrics_locked <- all_results %>%
filter(.metric %in% metrics_to_keep) %>%
inner_join(best_f1_config, by = c("model", ".config"))
summary_metrics_locked
metrics_to_keep <- c("accuracy", "precision", "recall", "f_meas", "roc_auc")
# For now: just show best per model per metric
summary_metrics <- all_results %>%
filter(.metric %in% c("accuracy", "precision", "recall", "f_meas", "roc_auc")) %>%
group_by(model, .metric) %>%
slice_max(mean, with_ties = FALSE) %>%
ungroup()
summary_metrics
view(summary_metrics)
# For now: just show best per model per metric
summary_metrics <- all_results %>%
filter(.metric %in% c("accuracy", "precision", "recall", "f_meas", "roc_auc")) %>%
group_by(model, .metric) %>%
slice_max(mean, with_ties = FALSE) %>%
ungroup() %>%
select(model, .metric, mean, std_err)
summary_metrics
summary_clean <- summary_metrics %>%
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
) %>%
rename_with(~ str_replace(., "_mean", ""), ends_with("_mean")) %>%
rename_with(~ paste0("std_err_", .), ends_with("_std_err")) %>%
select(model, accuracy, precision, recall, f_meas, roc_auc, std_err_f_meas) %>%
arrange(desc(f_meas))
summary_clean <- summary_metrics %>%
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
) %>%
# This gives you columns like: accuracy_mean, accuracy_std_err, f_meas_mean, f_meas_std_err, etc.
rename_with(~ str_replace(., "_mean$", ""), ends_with("_mean")) %>%
rename_with(~ str_replace(., "_std_err$", "std_err"), ends_with("_std_err")) %>%
# Now youâ€™ll have f_meas and f_measstd_err, so adjust the column name accordingly:
select(model, accuracy, precision, recall, f_meas, roc_auc, f_measstd_err) %>%
rename(std_err_f_meas = f_measstd_err) %>%
arrange(desc(f_meas))
summary_clean
# ---- Pivot to Wide Format and Clean ----
summary_wide <- summary_metrics |>
select(model, .metric, mean, std_err) |>
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
)
summary_wide
summary_clean <- summary_wide |>
rename_with(~ str_replace(., "_mean", ""), ends_with("_mean")) |>
rename_with(~ paste0("std_err_", .), ends_with("_std_err")) |>
select(model, accuracy, precision, recall, f_meas, roc_auc, std_err_f_meas)
# =============================
# Analysis: Attempt 1 Summary Table
# =============================
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(doMC)
library(pROC)
tidymodels_prefer()
set.seed(301)
# ---- Register Parallel Processing ----
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# ---- Load Data and Fits ----
load(here("attempt_1/results/data_split.rda"))
load(here("attempt_1/recipes/ks_recipes.rda"))
load(here("attempt_1/results/a_logistic_fit.rda"))
load(here("attempt_1/results/a_dtree_fit.rda"))
load(here("attempt_1/results/a_nbayes_fit.rda"))
load(here("attempt_1/results/a_knn_fit.rda"))
load(here("attempt_1/results/a_rulefit_fit.rda"))
# ---- Combine and Filter Metrics ----
all_results <- bind_rows(
collect_metrics(logistic_fit) |> mutate(model = "Logistic"),
collect_metrics(dtree_fit)    |> mutate(model = "Decision Tree"),
collect_metrics(knn_fit)      |> mutate(model = "KNN"),
collect_metrics(nbayes_fit)   |> mutate(model = "Naive Bayes"),
collect_metrics(rulefit_fit)  |> mutate(model = "RuleFit")
)
# ---- Keep only the best score per model per metric ----
metrics_to_keep <- c("accuracy", "precision", "recall", "f_meas", "roc_auc")
# ---- Keep only the best score per model per metric ----
metrics_to_keep <- c("accuracy", "precision", "recall", "f_meas", "roc_auc")
summary_metrics <- all_results |>
filter(.metric %in% metrics_to_keep) |>
group_by(model, .metric) |>
slice_max(mean, with_ties = FALSE) |>
ungroup() |>
select(model, .metric, mean, std_err)
# ---- Pivot to Wide Format and Clean ----
summary_clean <- summary_metrics |>
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
) |>
rename_with(~ str_replace(., "_mean$", ""), ends_with("_mean")) |>
rename_with(~ str_replace(., "_std_err$", "std_err"), ends_with("_std_err")) |>
select(model, accuracy, precision, recall, f_meas, roc_auc, f_measstd_err) |>
rename(std_err_f_meas = f_measstd_err)
# ---- Sort by F1-score ----
summary_final_attempt1 <- summary_clean |>
arrange(desc(f_meas))
# ---- Preview Final Table ----
print(summary_final_attempt1)
# ---- Save to CSV and RDA ----
save(summary_final_attempt1, file = here("attempt_1/results/attempt_1_summary_table.rda"))
# =============================
# Analysis: Attempt 2 Summary Table
# =============================
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(doMC)
library(pROC)
tidymodels_prefer()
set.seed(301)
# ---- Register Parallel Processing ----
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# ---- Load Data and Fits ----
load(here("attempt_2/results/data_split.rda"))
load(here("attempt_2/recipes/ks_recipes.rda"))
load(here("attempt_2/recipes/ks_recipes.rda"))
# =============================
# Analysis: Attempt 2 Summary Table
# =============================
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(doMC)
library(pROC)
tidymodels_prefer()
set.seed(301)
# ---- Register Parallel Processing ----
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# ---- Load Data and Fits ----
load(here("attempt_2/results/data_split.rda"))
load(here("attempt_2/results/a_logistic_fit.rda"))
# =============================
# Analysis: Attempt 2 Summary Table
# =============================
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(doMC)
library(pROC)
tidymodels_prefer()
set.seed(301)
# ---- Register Parallel Processing ----
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# ---- Load Data and Fits ----
load(here("attempt_2/results/data_split.rda"))
load(here("attempt_2/results/b_logistic_fit.rda"))
load(here("attempt_2/results/b_dtree_fit.rda"))
load(here("attempt_2/results/b_nbayes_fit.rda"))
load(here("attempt_2/results/b_knn_fit.rda"))
load(here("attempt_2/results/b_rulefit_fit.rda"))
# ---- Combine and Filter Metrics ----
all_results <- bind_rows(
collect_metrics(logistic_fit) |> mutate(model = "Logistic"),
collect_metrics(dtree_fit)    |> mutate(model = "Decision Tree"),
collect_metrics(knn_fit)      |> mutate(model = "KNN"),
collect_metrics(nbayes_fit)   |> mutate(model = "Naive Bayes"),
collect_metrics(rulefit_fit)  |> mutate(model = "RuleFit")
)
# ---- Keep only the best score per model per metric ----
metrics_to_keep <- c("accuracy", "precision", "recall", "f_meas", "roc_auc")
summary_metrics <- all_results |>
filter(.metric %in% metrics_to_keep) |>
group_by(model, .metric) |>
slice_max(mean, with_ties = FALSE) |>
ungroup() |>
select(model, .metric, mean, std_err)
# ---- Pivot to Wide Format and Clean ----
summary_clean <- summary_metrics |>
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
) |>
rename_with(~ str_replace(., "_mean$", ""), ends_with("_mean")) |>
rename_with(~ str_replace(., "_std_err$", "std_err"), ends_with("_std_err")) |>
select(model, accuracy, precision, recall, f_meas, roc_auc, f_measstd_err) |>
rename(std_err_f_meas = f_measstd_err)
# ---- Sort by F1-score ----
summary_final_attempt2 <- summary_clean |>
arrange(desc(f_meas))
# ---- Preview Final Table ----
print(summary_final_attempt2)
# ---- Save to CSV and RDA ----
save(summary_final_attempt2, file = here("attempt_2/results/attempt_2_summary_table.rda"))
# =============================
# Analysis: Attempt 3 Summary Table
# =============================
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(doMC)
library(pROC)
tidymodels_prefer()
set.seed(301)
# ---- Register Parallel Processing ----
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# ---- Load Data and Fits ----
load(here("attempt_3/results/data_split.rda"))
load(here("attempt_3/results/c_logistic_fit.rda"))
load(here("attempt_3/results/c_dtree_fit.rda"))
load(here("attempt_3/results/c_nbayes_fit.rda"))
load(here("attempt_3/results/c_knn_fit.rda"))
load(here("attempt_3/results/c_rulefit_fit.rda"))
# ---- Combine and Filter Metrics ----
all_results <- bind_rows(
collect_metrics(logistic_fit) |> mutate(model = "Logistic"),
collect_metrics(dtree_fit)    |> mutate(model = "Decision Tree"),
collect_metrics(knn_fit)      |> mutate(model = "KNN"),
collect_metrics(nbayes_fit)   |> mutate(model = "Naive Bayes"),
collect_metrics(rulefit_fit)  |> mutate(model = "RuleFit")
)
# ---- Keep only the best score per model per metric ----
metrics_to_keep <- c("accuracy", "precision", "recall", "f_meas", "roc_auc")
summary_metrics <- all_results |>
filter(.metric %in% metrics_to_keep) |>
group_by(model, .metric) |>
slice_max(mean, with_ties = FALSE) |>
ungroup() |>
select(model, .metric, mean, std_err)
# ---- Pivot to Wide Format and Clean ----
summary_clean <- summary_metrics |>
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
) |>
rename_with(~ str_replace(., "_mean$", ""), ends_with("_mean")) |>
rename_with(~ str_replace(., "_std_err$", "std_err"), ends_with("_std_err")) |>
select(model, accuracy, precision, recall, f_meas, roc_auc, f_measstd_err) |>
rename(std_err_f_meas = f_measstd_err)
# ---- Sort by F1-score ----
summary_final_attempt3 <- summary_clean |>
arrange(desc(f_meas))
# ---- Preview Final Table ----
print(summary_final_attempt3)
# ---- Save RDA ----
save(summary_final_attempt3, file = here("attempt_3/results/attempt_3_summary_table.rda"))
# =============================
# Analysis: Attempt 3 Summary Table
# =============================
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(doMC)
library(pROC)
tidymodels_prefer()
set.seed(301)
# ---- Register Parallel Processing ----
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# ---- Load Data and Fits ----
load(here("attempt_4/results/data_split.rda"))
load(here("attempt_4/results/d_logistic_fit.rda"))
load(here("attempt_4/results/d_dtree_fit.rda"))
load(here("attempt_4/results/d_nbayes_fit.rda"))
load(here("attempt_4/results/d_knn_fit.rda"))
load(here("attempt_4/results/d_rulefit_fit.rda"))
# ---- Combine and Filter Metrics ----
all_results <- bind_rows(
collect_metrics(logistic_fit) |> mutate(model = "Logistic"),
collect_metrics(dtree_fit)    |> mutate(model = "Decision Tree"),
collect_metrics(knn_fit)      |> mutate(model = "KNN"),
collect_metrics(nbayes_fit)   |> mutate(model = "Naive Bayes"),
collect_metrics(rulefit_fit)  |> mutate(model = "RuleFit")
)
# ---- Keep only the best score per model per metric ----
metrics_to_keep <- c("accuracy", "precision", "recall", "f_meas", "roc_auc")
summary_metrics <- all_results |>
filter(.metric %in% metrics_to_keep) |>
group_by(model, .metric) |>
slice_max(mean, with_ties = FALSE) |>
ungroup() |>
select(model, .metric, mean, std_err)
# ---- Pivot to Wide Format and Clean ----
summary_clean <- summary_metrics |>
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
) |>
rename_with(~ str_replace(., "_mean$", ""), ends_with("_mean")) |>
rename_with(~ str_replace(., "_std_err$", "std_err"), ends_with("_std_err")) |>
select(model, accuracy, precision, recall, f_meas, roc_auc, f_measstd_err) |>
rename(std_err_f_meas = f_measstd_err)
# ---- Sort by F1-score ----
summary_final_attempt4 <- summary_clean |>
arrange(desc(f_meas))
# ---- Preview Final Table ----
print(summary_final_attempt4)
# ---- Save RDA ----
save(summary_final_attempt4, file = here("attempt_4/results/attempt_4_summary_table.rda"))
# =============================
# Analysis: Attempt 3 Summary Table
# =============================
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(doMC)
library(pROC)
tidymodels_prefer()
set.seed(301)
# ---- Register Parallel Processing ----
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# ---- Load Data and Fits ----
load(here("attempt_5/results/data_split.rda"))
load(here("attempt_5/results/e_logistic_fit.rda"))
load(here("attempt_5/results/e_dtree_fit.rda"))
load(here("attempt_5/results/e_nbayes_fit.rda"))
load(here("attempt_5/results/e_knn_fit.rda"))
load(here("attempt_5/results/e_rulefit_fit.rda"))
# ---- Combine and Filter Metrics ----
all_results <- bind_rows(
collect_metrics(logistic_fit) |> mutate(model = "Logistic"),
collect_metrics(dtree_fit)    |> mutate(model = "Decision Tree"),
collect_metrics(knn_fit)      |> mutate(model = "KNN"),
collect_metrics(nbayes_fit)   |> mutate(model = "Naive Bayes"),
collect_metrics(rulefit_fit)  |> mutate(model = "RuleFit")
)
# ---- Keep only the best score per model per metric ----
metrics_to_keep <- c("accuracy", "precision", "recall", "f_meas", "roc_auc")
summary_metrics <- all_results |>
filter(.metric %in% metrics_to_keep) |>
group_by(model, .metric) |>
slice_max(mean, with_ties = FALSE) |>
ungroup() |>
select(model, .metric, mean, std_err)
# ---- Pivot to Wide Format and Clean ----
summary_clean <- summary_metrics |>
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
) |>
rename_with(~ str_replace(., "_mean$", ""), ends_with("_mean")) |>
rename_with(~ str_replace(., "_std_err$", "std_err"), ends_with("_std_err")) |>
select(model, accuracy, precision, recall, f_meas, roc_auc, f_measstd_err) |>
rename(std_err_f_meas = f_measstd_err)
# ---- Sort by F1-score ----
summary_final_attempt5 <- summary_clean |>
arrange(desc(f_meas))
# ---- Preview Final Table ----
print(summary_final_attempt5)
# ---- Save RDA ----
save(summary_final_attempt5, file = here("attempt_5/results/attempt_5_summary_table.rda"))
