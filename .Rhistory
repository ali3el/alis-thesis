# RuleFit
# Load packages ----
library(tidyverse)
library(tidymodels)
library(here)
library(xrf)
# Handle common conflicts
tidymodels_prefer()
# Set seed
set.seed(301)
# Load preprocessed data and recipe ----
load(here("results/data_split.rda"))
install.packages(c("abind", "arrow", "askpass", "bigD", "bit", "bit64", "bitops", "boot", "broom", "bslib", "checkmate", "cli", "clock", "cluster", "colorspace", "commonmark", "corrplot", "cpp11", "crayon", "credentials", "curl", "data.table", "DBI", "dials", "digest", "distributional", "downlit", "duckdb", "e1071", "earth", "evaluate", "fontawesome", "foreign", "fs", "future", "future.apply", "gert", "ggrepel", "glue", "gt", "gtable", "hardhat", "hexbin", "httr2", "igraph", "inline", "ipred", "jsonlite", "kernlab", "knitr", "Lahman", "later", "lhs", "lme4", "loo", "lubridate", "maps", "markdown", "matrixStats", "minqa", "modeldata", "modelenv", "nlme", "nloptr", "openintro", "openssl", "openxlsx", "parallelly", "patchwork", "pkgbuild", "pkgdown", "pkgload", "PKI", "plotmo", "posterior", "prodlim", "profvis", "progressr", "promises", "ps", "QuickJSR", "R.oo", "r4ds.tutorials", "ragg", "ranger", "RANN", "raster", "Rcpp", "RcppEigen", "RcppParallel", "reactR", "recipes", "renv", "reprex", "reticulate", "rlang", "rmarkdown", "roxygen2", "rsconnect", "RSQLite", "rstudioapi", "shiny", "slider", "stacks", "StanHeaders", "survival", "sys", "textshaping", "timeDate", "tinytex", "tutorial.helpers", "usdata", "usethis", "uuid", "V8", "VGAM", "waldo", "withr", "writexl", "xfun", "xgboost", "xts", "yaml"))
# data splitting
# load packages ----
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
library(janitor)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
combined_data <- read_csv(here("data_combined/combined_nhis_data.csv"))
# data splitting
# load packages ----
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
library(janitor)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
combined_data <- read_csv(here("data_combined/combined_nhis_data.csv"))
# data splitting
# load packages ----
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
library(janitor)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
combined_data <- read_csv(here("data_combined/combined_nhis_data.csv"))
# data split
data_split <- initial_split(combined_data,
prop = 0.80,
strat = depression_ever,
breaks = 4)
data_train <- data_split |> training()
data_test <- data_split |> testing()
data_fold <- vfold_cv(data_train, v = 5, repeats = 3,
strata = depression_ever)
save(data_split, data_train, data_test, data_fold, file = here("results/data_split.rda"))
# Setup pre-processing/recipes - KITCHEN SINK
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
load(here("results/data_split.rda"))
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
# kitchen sink recipe
## for non tree (parametric) models
ks_recipe <- recipe(depression_ever ~ ., data = data_train) |>
step_rm(year) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
# Convert nominal predictors to dummy variables
step_dummy(all_nominal_predictors()) |>
# Remove zero-variance predictors
step_zv(all_predictors()) |>
# Normalize predictors
step_normalize(all_predictors())
prep(ks_recipe) |>
bake(new_data = NULL)
## for  tree (non-parametric) models
ks_recipe_t <- recipe(depression_ever ~ ., data = data_train) |>
# Remove other unnecessary variables
step_rm(year) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
step_zv(all_predictors()) |>
step_normalize(all_predictors())
prep(ks_recipe_t) |>
bake(new_data = NULL)
save(ks_recipe, ks_recipe_t, file = "recipes/ks_recipes.rda")
# KNN
# # load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
load(here("results/data_split.rda"))
load(here("recipes/ks_recipes.rda"))
library(doMC)
registerDoMC(cores = parallel::detectCores(logical =TRUE))
# model specifications ----
knn_model <-
nearest_neighbor(
mode = "classification",
neighbors = tune()) |>
set_engine("kknn")
# define workflows ----
knn_workflow <- workflow() |>
add_model(knn_model) |>
add_recipe(ks_recipe)
# hyperparameter tuning values ----
knn_params <- extract_parameter_set_dials(knn_model)
knn_grid <- grid_regular(knn_params, levels = 5)
# fit workflows/models ----
knn_fit <- tune_grid(knn_workflow,
data_fold,
grid = knn_grid,
control = control_grid(save_workflow = TRUE))
# data splitting
# load packages ----
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
library(janitor)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
combined_data <- read_csv(here("data_combined/combined_nhis_data.csv"))
# data split
data_split <- initial_split(combined_data,
prop = 0.80,
strat = depression_ever,
breaks = 4)
data_train <- data_split |> training()
data_test <- data_split |> testing()
data_fold <- vfold_cv(data_train, v = 5,
strata = depression_ever)
save(data_split, data_train, data_test, data_fold, file = here("results/data_split.rda"))
# Setup pre-processing/recipes - KITCHEN SINK
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
load(here("results/data_split.rda"))
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
# kitchen sink recipe
## for non tree (parametric) models
ks_recipe <- recipe(depression_ever ~ ., data = data_train) |>
step_rm(year) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
# Convert nominal predictors to dummy variables
step_dummy(all_nominal_predictors()) |>
# Remove zero-variance predictors
step_zv(all_predictors()) |>
# Normalize predictors
step_normalize(all_predictors())
prep(ks_recipe) |>
bake(new_data = NULL)
## for  tree (non-parametric) models
ks_recipe_t <- recipe(depression_ever ~ ., data = data_train) |>
# Remove other unnecessary variables
step_rm(year) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
step_zv(all_predictors()) |>
step_normalize(all_predictors())
prep(ks_recipe_t) |>
bake(new_data = NULL)
save(ks_recipe, ks_recipe_t, file = "recipes/ks_recipes.rda")
# Logistic Regression
# # load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
load(here("results/data_split.rda"))
load(here("recipes/ks_recipes.rda"))
library(doMC)
registerDoMC(cores = parallel::detectCores(logical =TRUE))
# model specifications ----
logistic_model <- logistic_reg() |>
set_engine("glm") |>
set_mode("classification")
# define workflows ----
logistic_workflow <- workflow() |>
add_model(logistic_model) |>
add_recipe(ks_recipe)
# fit workflows/models ----
logistic_fit <- fit_resamples(logistic_workflow,
resamples = data_fold,
control = control_resamples(
save_workflow = TRUE,
parallel_over = "everything"))
# when tuning we need to make a grid
# write out results (fitted/trained workflows) ----
save(logistic_fit, file = "results/a_logistic_fit.rda")
source("~/Desktop/Thesis/3a_knn.R", echo=TRUE)
# Setup pre-processing/recipes - KITCHEN SINK
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
load(here("results/data_split.rda"))
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
# kitchen sink recipe
## for non tree (parametric) models
ks_recipe <- recipe(depression_ever ~ ., data = data_train) |>
step_rm(year) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_novel(all_nominal_predictors()) |>
# Convert nominal predictors to dummy variables
step_dummy(all_nominal_predictors()) |>
# Remove zero-variance predictors
step_zv(all_predictors()) |>
# Normalize predictors
step_normalize(all_predictors())
prep(ks_recipe) |>
bake(new_data = NULL)
## for  tree (non-parametric) models
ks_recipe_t <- recipe(depression_ever ~ ., data = data_train) |>
# Remove other unnecessary variables
step_rm(year) |>
step_impute_mode(all_nominal_predictors()) |>  # For categorical variables
step_impute_median(all_numeric_predictors()) |>  # For numeric variables
step_novel(all_nominal_predictors()) |>
step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
step_zv(all_predictors()) |>
step_normalize(all_predictors())
prep(ks_recipe_t) |>
bake(new_data = NULL)
save(ks_recipe, ks_recipe_t, file = "recipes/ks_recipes.rda")
# Logistic Regression
# # load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# handle common conflicts
tidymodels_prefer()
# set seed
set.seed(301)
load(here("results/data_split.rda"))
load(here("recipes/ks_recipes.rda"))
library(doMC)
registerDoMC(cores = parallel::detectCores(logical =TRUE))
# model specifications ----
logistic_model <- logistic_reg() |>
set_engine("glm") |>
set_mode("classification")
# define workflows ----
logistic_workflow <- workflow() |>
add_model(logistic_model) |>
add_recipe(ks_recipe)
# fit workflows/models ----
logistic_fit <- fit_resamples(logistic_workflow,
resamples = data_fold,
control = control_resamples(
save_workflow = TRUE,
parallel_over = "everything"))
# when tuning we need to make a grid
# write out results (fitted/trained workflows) ----
save(logistic_fit, file = "results/a_logistic_fit.rda")
show_engines('linear_reg')
install.packages("rules")
# RuleFit
# Load packages ----
library(tidyverse)
library(tidymodels)
library(here)
library(xrf)
library(rules)
# Handle common conflicts
tidymodels_prefer()
# Set seed
set.seed(301)
# Load preprocessed data and recipe ----
load(here("results/data_split.rda"))
load(here("recipes/ks_recipes.rda"))
# Enable parallel processing ----
library(doMC)
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# Set RuleFit model specification
rulefit_model <- rule_fit(
model = "classification",
penalty = tune(),
mixture = tune()
) |>
set_engine("xrf")
# Set RuleFit model specification
rulefit_model <- rule_fit(
model = "classification",
penalty = tune()
) |>
set_engine("xrf")
# Set RuleFit model specification
rulefit_model <- rule_fit(
mode = "classification",
penalty = tune(),
mixture = tune()
) |>
set_engine("xrf")
# Set RuleFit model specification
rulefit_model <- rule_fit(
mode = "classification",
penalty = tune()
) |>
set_engine("xrf")
rulefit_workflow <- workflow() |>
add_model(rulefit_model) |>
add_recipe(ks_recipe_t)
rulefit_params <- parameters(rulefit_model)
rulefit_grid <- grid_regular(rulefit_params, levels = 3)
rulefit_params <- parameters(rulefit_model)
rulefit_grid <- grid_regular(rulefit_params, levels = 3)
load(here("recipes/ks_recipes.rda"))
# Enable parallel processing ----
library(doMC)
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# Set RuleFit model specification
rulefit_model <- rule_fit(
mode = "classification",
penalty = tune()
) |>
set_engine("xrf")
rulefit_workflow <- workflow() |>
add_model(rulefit_model) |>
add_recipe(ks_recipe_t)
rulefit_params <- extract_parameter_set_dials(rulefit_model)
rulefit_grid <- grid_regular(rulefit_params, levels = 3)
rulefit_fit <- tune_grid(
rulefit_workflow,
resamples = data_fold, # `data_fold` is my cross-validation folds
grid = rulefit_grid,
control = control_grid(save_workflow = TRUE)
)
# decision tree
# Load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# Handle common conflicts
tidymodels_prefer()
# Set seed
set.seed(301)
# Load preprocessed data and recipe ----
load(here("results/data_split.rda"))
load(here("recipes/ks_recipes.rda"))
# Enable parallel processing ----
library(doMC)
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# Model specifications ----
dtree_model <-
decision_tree(
mode = "classification",
cost_complexity = tune(),   # Controls pruning
tree_depth = tune(),        # Max depth of tree
min_n = tune()              # Minimum samples required for a split
) |>
set_engine("rpart")           # Use the rpart engine
# Define workflow ----
dtree_workflow <- workflow() |>
add_model(dtree_model) |>
add_recipe(ks_recipe_t)
dtree_params <- extract_parameter_set_dials(dtree_model)
dtree_grid <- grid_regular(dtree_params, levels = 5)
dtree_fit <- tune_grid(
dtree_workflow,
resamples = data_fold,        # Assuming `data_fold` is your cross-validation folds
grid = dtree_grid,
control = control_grid(save_workflow = TRUE)
)
save(dtree_fit, file = "results/a_dtree_fit.rda")
