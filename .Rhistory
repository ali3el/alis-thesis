y = "F1 Score", x = "Platform"
) +
theme_minimal()
# ---- Plot F1-Score Comparison ----
ggplot(combined_best, aes(x = recipe, y = f_meas, fill = model)) +
geom_col(position = "dodge") +
facet_wrap(~ platform) +
labs(
title = "F1 Score by Recipe and Model (Local vs Quest)",
y = "F1 Score", x = "Recipe"
) +
theme_minimal()
library(patchwork)
final_plot <- f1_plot + roc_plot + accuracy_plot +
plot_layout(ncol = 1) +
plot_annotation(title = "Performance Metrics by Recipe and Platform")
# ---- Plot F1-Score Comparison ----
f1_plot <- ggplot(combined_best, aes(x = recipe, y = f_meas, fill = model)) +
geom_col(position = "dodge") +
facet_wrap(~ platform) +
labs(
title = "F1 Score by Recipe and Model (Local vs Quest)",
y = "F1 Score", x = "Recipe"
) +
theme_minimal()
final_plot <- f1_plot + roc_plot + accuracy_plot +
plot_layout(ncol = 1) +
plot_annotation(title = "Performance Metrics by Recipe and Platform")
# ---- Load Libraries ----
library(tidyverse)
library(here)
library(patchwork)  # for combining plots
# ---- Load Data ----
local_best <- read_csv(here("master_comparison/local_best_models.csv"))
# ---- Load Libraries ----
library(tidyverse)
library(here)
# ---- Load Data ----
load(here("master_comparison/local_model_results.rda"))
load(here("master_comparison/quest_model_results.rda"))
# ---- Combine Both ----
combined_best <- bind_rows(local_best_models, quest_best_models)
# ---- F1 Score Plot ----
f1_plot <- ggplot(combined_best, aes(x = recipe, y = f_meas, fill = model)) +
geom_col(position = "dodge") +
facet_wrap(~ platform) +
labs(
title = "F1 Score by Recipe and Model (Local vs Quest)",
x = "Recipe", y = "F1 Score"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
f1_plot
# ---- ROC AUC Plot ----
roc_plot <- ggplot(combined_best, aes(x = recipe, y = roc_auc, fill = model)) +
geom_col(position = "dodge") +
facet_wrap(~ platform) +
labs(
title = "ROC AUC by Recipe and Model (Local vs Quest)",
x = "Recipe", y = "ROC AUC"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
roc_plot
# ---- Accuracy Plot ----
accuracy_plot <- ggplot(combined_best, aes(x = recipe, y = accuracy, fill = model)) +
geom_col(position = "dodge") +
facet_wrap(~ platform) +
labs(
title = "Accuracy by Recipe and Model (Local vs Quest)",
x = "Recipe", y = "Accuracy"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
accuracy_plot
# ---- Combined Vertical Layout (Optional) ----
combined_plot <- f1_plot / roc_plot / accuracy_plot +
plot_annotation(title = "Model Performance Metrics by Recipe and Platform")
combined_plot
# ---- Combine and Clean ----
combined_best <- bind_rows(local_best_models, quest_best_models) |>
mutate(
model = str_to_title(model),      # Clean model labels
recipe = factor(recipe, levels = c("Kitchen Sink", "M1", "M2", "M3", "M4"))
)
# ---- Combine and Clean ----
combined_best <- bind_rows(local_best_models, quest_best_models) |>
mutate(
model = str_to_title(model),      # Clean model labels
recipe = factor(recipe, levels = c("Kitchen Sink", "M1", "M2", "M3", "M4"))
)
# ---- Load Libraries ----
library(tidyverse)
library(here)
library(patchwork)
# ---- Load Data ----
load(here("master_comparison/local_model_results.rda"))
load(here("master_comparison/quest_model_results.rda"))
# ---- Combine and Clean ----
combined_best <- bind_rows(local_best_models, quest_best_models) |>
mutate(
model = str_to_title(model),      # Clean model labels
recipe = factor(recipe, levels = c("Kitchen Sink", "M1", "M2", "M3", "M4"))
)
# ---- F1 Score Plot ----
f1_plot <- ggplot(combined_best, aes(x = recipe, y = f_meas, fill = model)) +
geom_col(position = "dodge") +
facet_wrap(~ platform) +
labs(
title = "F1 Score by Recipe and Model (Local vs Quest)",
x = "Recipe", y = "F1 Score"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# ---- ROC AUC Plot ----
roc_plot <- ggplot(combined_best, aes(x = recipe, y = roc_auc, fill = model)) +
geom_col(position = "dodge") +
facet_wrap(~ platform) +
labs(
title = "ROC AUC by Recipe and Model (Local vs Quest)",
x = "Recipe", y = "ROC AUC"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# ---- Accuracy Plot ----
accuracy_plot <- ggplot(combined_best, aes(x = recipe, y = accuracy, fill = model)) +
geom_col(position = "dodge") +
facet_wrap(~ platform) +
labs(
title = "Accuracy by Recipe and Model (Local vs Quest)",
x = "Recipe", y = "Accuracy"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# ---- Combined Vertical Layout (Optional) ----
combined_plot <- f1_plot / roc_plot / accuracy_plot +
plot_annotation(title = "Model Performance Metrics by Recipe and Platform")
# ---- Save Individual Plots ----
ggsave(here("visualizations/plots/f1_comparison_by_model_recipe.png"), f1_plot, width = 10, height = 6, dpi = 300)
ggsave(here("visualizations/plots/rocauc_comparison_by_model_recipe.png"), roc_plot, width = 10, height = 6, dpi = 300)
ggsave(here("visualizations/plots/accuracy_comparison_by_model_recipe.png"), accuracy_plot, width = 10, height = 6, dpi = 300)
# ---- Save Combined Plot ----
ggsave(here("visualizations/plots/combined_model_performance_metrics.png"), combined_plot, width = 12, height = 14, dpi = 300)
# ---- Save as .rda for reuse ----
save(f1_plot, roc_plot, accuracy_plot, combined_plot, file = here("visualizations/plots/combined_model_comparison_plots.rda"))
# ---- Load Libraries ----
library(tidyverse)
library(here)
library(patchwork)
# ---- Load Data ----
load(here("master_comparison/local_model_results.rda"))   # loads local_best_models
load(here("master_comparison/quest_model_results.rda"))   # loads quest_best_models
# ---- Combine & Clean ----
combined_best <- bind_rows(local_best_models, quest_best_models) |>
mutate(
model = str_to_title(model),  # standardize labels: "knn" → "Knn"
recipe = factor(recipe, levels = c("Kitchen Sink", "M1", "M2", "M3", "M4"))
)
# ---- Create Plot Function (DRY principle) ----
create_metric_plot <- function(data, metric, metric_label) {
ggplot(data, aes(x = recipe, y = .data[[metric]], fill = model)) +
geom_col(position = "dodge") +
facet_wrap(~ platform) +
labs(
title = glue::glue("{metric_label} by Recipe and Model (Local vs Quest)"),
x = "Recipe", y = metric_label
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
# ---- Generate Plots ----
f1_plot       <- create_metric_plot(combined_best, "f_meas", "F1 Score")
roc_plot      <- create_metric_plot(combined_best, "roc_auc", "ROC AUC")
accuracy_plot <- create_metric_plot(combined_best, "accuracy", "Accuracy")
# ---- Combine Plots Vertically ----
combined_plot <- f1_plot / roc_plot / accuracy_plot +
plot_annotation(title = "Model Performance Metrics by Recipe and Platform")
# ---- Save Individual Plots ----
ggsave(here("visualizations/plots/f1_comparison_by_model_recipe.png"),       f1_plot,       width = 10, height = 6, dpi = 300)
ggsave(here("visualizations/plots/rocauc_comparison_by_model_recipe.png"),   roc_plot,      width = 10, height = 6, dpi = 300)
ggsave(here("visualizations/plots/accuracy_comparison_by_model_recipe.png"), accuracy_plot, width = 10, height = 6, dpi = 300)
# ---- Save Combined Plot ----
ggsave(here("visualizations/plots/combined_model_performance_metrics.png"), combined_plot, width = 12, height = 14, dpi = 300)
# ---- Save All as .rda for Reuse ----
save(f1_plot, roc_plot, accuracy_plot, combined_plot,
file = here("visualizations/plots/combined_model_comparison_plots.rda"))
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
install.packages("DALEXtra")
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))       # loads ks tree recipe
load(here("model_explainability/quest_files/ks_dtree-fit"))     # loads dtree fit
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))       # loads ks tree recipe
load(here("model_explainability/quest_files/ks_dtree_fit"))     # loads dtree fit
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))     #
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))       # loads ks tree recipe
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))     # loads dtree fit
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))       # loads ks tree recipe
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))     # loads dtree fit
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))       # loads data split train
load(here("model_explainability/quest_files/ks_recipes.rda")) # loads ks tree recipe
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))     # loads dtree fit
# Apply recipe to training data
# Apply recipe to training data
baked_train <- bake(ks_recipe_t, new_data = NULL)
ks_recipe_t <- prep(ks_recipe_t)
baked_train <- bake(ks_recipe_t, new_data = NULL)
# Build the Explainer
explainer_ks_tree <- explain_tidymodels(
model = dtree_fit,
data = baked_train |> select(-depression),  # predictors only
y = baked_train$depression,                  # outcome variable
label = "Quest – Kitchen Sink – Decision Tree",
verbose = FALSE
)
dtree_fit
# Apply recipe to training data
ks_recipe_t <- prep(ks_recipe_t)
baked_train <- bake(ks_recipe_t, new_data = NULL)
# Add depression column from training split
baked_train$depression <- data_train$depression
# Build the Explainer
explainer_ks_tree <- explain_tidymodels(
model = dtree_fit,
data = baked_train |> select(-depression),  # predictors only
y = baked_train$depression,                  # outcome variable
label = "Quest – Kitchen Sink – Decision Tree",
verbose = FALSE
)
ks_recipe_t <- prep(ks_recipe_t)
baked_train <- baked_train |>
mutate(depression = data_train$depression)
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))       # loads data split train
load(here("model_explainability/quest_files/ks_recipes.rda")) # loads ks tree recipe
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))     # loads dtree fit
# Apply recipe to training data
ks_recipe_t <- prep(ks_recipe_t)
baked_train <- bake(ks_recipe_t, new_data = NULL)
# Add depression column from training split
baked_train$depression <- data_train$depression_ever
# Build the Explainer
explainer_ks_tree <- explain_tidymodels(
model = dtree_fit,
data = baked_train |> select(-depression_ever),  # predictors only
y = baked_train$depression_ever,                  # outcome variable
label = "Quest – Kitchen Sink – Decision Tree",
verbose = FALSE
)
explainer_ks_tree
vip_ks_tree <- model_parts(explainer_ks_tree, type = "difference")
# ---- Due to tuning
# 1. Select best parameters by F1
best_params <- select_best(dtree_fit, metric = "f_meas")
# 2. Finalize your workflow (replace this with your actual workflow object)
final_wf <- finalize_workflow(dtree_wf, best_params)
# ---- Due to tuning
dtree_wf <- extract_workflow(dtree_fit)
# 1. Select best parameters by F1
best_params <- select_best(dtree_fit, metric = "f_meas")
# 1. Get best hyperparameters
best_params <- select_best(dtree_fit, metric = "f_meas")
# 2. Finalize the workflow
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
# 3. Fit it to your full training data
final_dtree_fit <- fit(final_dtree_wf, data = baked_train)
# 3. Fit it to your full training data
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
final_dtree_fit
# Bake it again manually for explainability
ks_recipe_t <- prep(ks_recipe_t)
baked_train <- bake(prep(ks_recipe_t), new_data = NULL)
baked_train$depression_ever <- data_train$depression_ever
# Build explainer
explainer_ks_tree <- explain_tidymodels(
model = final_dtree_fit,
data = baked_train |> select(-depression_ever),
y = baked_train$depression_ever,
label = "Quest – Kitchen Sink – Decision Tree",
verbose = FALSE
)
vip_ks_tree <- model_parts(explainer_ks_tree, type = "difference")
# Plot
plot(vip_ks_tree)
library(tidyverse)
library(tidymodels)
library(here)
library(DALEXtra)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))       # data_split
load(here("model_explainability/quest_files/ks_recipes.rda"))          # ks_recipe_t
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))        # dtree_fit (tuning result)
# ---- Finalize & Fit the Model ----
dtree_wf <- extract_workflow(dtree_fit)
# 1. Get best hyperparameters
best_params <- select_best(dtree_fit, metric = "f_meas")
# 2. Finalize the workflow
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
# 3. Get original training data
data_train <- training(data_split)
# 4. Fit final model to full training data
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
# ---- Prepare Data for Explainability ----
ks_recipe_t <- prep(ks_recipe_t)
baked_train <- bake(ks_recipe_t, new_data = NULL)
baked_train$depression_ever <- data_train$depression_ever
# ---- Build the Explainer ----
explainer_ks_tree <- explain_tidymodels(
model = final_dtree_fit,
data = baked_train |> select(-depression_ever),
y = baked_train$depression_ever,
label = "Quest – Kitchen Sink – Decision Tree",
verbose = FALSE
)
# ---- Variable Importance (Global) ----
vip_ks_tree <- model_parts(explainer_ks_tree, type = "difference")
# ---- Build the Explainer ----
explainer_ks_tree <- explain_tidymodels(
model = final_dtree_fit,
data = data_train |> select(-depression_ever),
y = baked_train$depression_ever,
label = "Quest – Kitchen Sink – Decision Tree",
verbose = FALSE
)
# ---- Variable Importance (Global) ----
vip_ks_tree <- model_parts(explainer_ks_tree, type = "difference")
# ---- Variable Importance (Global) ----
vip_ks_tree <- model_parts(explainer_ks_tree, type = "difference")
# ---- Variable Importance (Global) ----
vip_ks_tree <- model_parts(explainer_ks_tree, type = "raw")
library(vip)
# ---- Variable Importance (Global) ----
vip_ks_tree <- model_parts(explainer_ks_tree, type = "difference")
# ---- Variable Importance (Global) ----
vip_ks_tree <- model_parts(explainer_ks_tree, type = "raw")
# Extract the fitted engine (actual decision tree object)
tree_model <- extract_fit_parsnip(final_dtree_fit)$fit
# ---- Variable Importance (Global) ----
vip(tree_model, num_features = 15,
bar = TRUE,
aesthetics = list(fill = "steelblue"),
title = "Variable Importance – Decision Tree (Quest – Kitchen Sink)")
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(vip)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))   # data_split
load(here("model_explainability/quest_files/ks_recipes.rda"))      # ks_recipe_t
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))    # dtree_fit (tuning result)
# ---- Finalize & Fit the Model ----
dtree_wf <- extract_workflow(dtree_fit)
# 1. Get best hyperparameters
best_params <- select_best(dtree_fit, metric = "f_meas")
# 2. Finalize the workflow
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
# 3. Get original training data
data_train <- training(data_split)
# 4. Fit final model to full training data
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
# ---- Extract fitted tree model for explainability ----
tree_model <- extract_fit_parsnip(final_dtree_fit)$fit
# ---- Create VIP Plot ----
vip_plot <- vip(tree_model,
num_features = 15,
bar = TRUE,
aesthetics = list(fill = "steelblue"),
title = "Variable Importance – Decision Tree (Quest – Kitchen Sink)")
# ---- Save Plot ----
ggsave(
filename = here("model_explainability/results/vip_dtree_quest_ks.png"),
plot = vip_plot,
width = 10, height = 6,
dpi = 300
)
save(vip_plot, here("model_explainability/results/vip_dtree_quest_ks.rda"))
save(vip_plot, file = here("model_explainability/results/vip_dtree_quest_ks.rda"))
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(vip)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/m3_data_split.rda"))   # data_split
load(here("model_explainability/quest_files/m3_recipes.rda"))      # m3_recipe_t
load(here("model_explainability/quest_files/3c_dtree_fit.rda"))
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/m3_data_split.rda"))   # data_split
load(here("model_explainability/quest_files/m3_recipes.rda"))      # m3_recipe_t
load(here("model_explainability/quest_files/m3_dtree_fit.rda"))    #
# ---- Finalize & Fit the Model ----
dtree_wf <- extract_workflow(dtree_fit_c)
# 1. Get best hyperparameters
best_params <- select_best(dtree_fit_c, metric = "f_meas")
# 2. Finalize the workflow
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
# 3. Get original training data
data_train <- training(data_split)
# 4. Fit final model to full training data
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
# ---- Extract fitted tree model for explainability ----
tree_model <- extract_fit_parsnip(final_dtree_fit)$fit
# ---- Create VIP Plot ----
vip_plot <- vip(tree_model,
num_features = 15,
bar = TRUE,
aesthetics = list(fill = "steelblue"),
title = "Variable Importance – Decision Tree (Quest – M3)")
# ---- Create VIP Plot ----
vip_plot_m3 <- vip(tree_model,
num_features = 15,
bar = TRUE,
aesthetics = list(fill = "steelblue"),
title = "Variable Importance – Decision Tree (Quest – M3)")
vip_plot_m3
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(vip)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/m3_data_split.rda"))   # data_split
load(here("model_explainability/quest_files/m3_recipes.rda"))      # m3_recipe_t
load(here("model_explainability/quest_files/m3_dtree_fit.rda"))    # dtree_fit (tuning result)
# ---- Finalize & Fit the Model ----
dtree_wf <- extract_workflow(dtree_fit_c)
# 1. Get best hyperparameters
best_params <- select_best(dtree_fit_c, metric = "f_meas")
# 2. Finalize the workflow
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
# 3. Get original training data
data_train <- training(data_split)
# 4. Fit final model to full training data
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
# ---- Extract fitted tree model for explainability ----
tree_model <- extract_fit_parsnip(final_dtree_fit)$fit
# ---- Create VIP Plot ----
vip_plot_m3 <- vip(tree_model,
num_features = 15,
bar = TRUE,
aesthetics = list(fill = "steelblue"),
title = "Variable Importance – Decision Tree (Quest – M3)")
# ---- Save Plot ----
ggsave(
filename = here("model_explainability/results/vip_dtree_quest_m3.png"),
plot = vip_plot_m3,
width = 10, height = 6,
dpi = 300
)
save(vip_plot_m3, file = here("model_explainability/results/vip_dtree_quest_m3.rda"))
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(vip)
# ---- Load Trained Objects ----
load(here("model_explainability/quest_files/ks_data_split.rda"))   # data_split
load(here("model_explainability/quest_files/ks_recipes.rda"))      # ks_recipe_t
load(here("model_explainability/quest_files/ks_dtree_fit.rda"))    # dtree_fit (tuning result)
# ---- Finalize & Fit the Model ----
dtree_wf <- extract_workflow(dtree_fit)
# 1. Get best hyperparameters
best_params <- select_best(dtree_fit, metric = "f_meas")
# 2. Finalize the workflow
final_dtree_wf <- finalize_workflow(dtree_wf, best_params)
# 3. Get original training data
data_train <- training(data_split)
# 4. Fit final model to full training data
final_dtree_fit <- fit(final_dtree_wf, data = data_train)
# ---- Extract fitted tree model for explainability ----
tree_model <- extract_fit_parsnip(final_dtree_fit)$fit
# ---- Create VIP Plot ----
vip_plot_ks <- vip(tree_model,
num_features = 15,
bar = TRUE,
aesthetics = list(fill = "steelblue"),
title = "Variable Importance – Decision Tree (Quest – Kitchen Sink)")
# ---- Save Plot ----
ggsave(
filename = here("model_explainability/results/vip_dtree_quest_ks.png"),
plot = vip_plot_ks,
width = 10, height = 6,
dpi = 300
)
save(vip_plot_ks, file = here("model_explainability/results/vip_dtree_quest_ks.rda"))
