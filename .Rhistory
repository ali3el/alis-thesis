load(here("attempt_5/results/e_rulefit_fit.rda"))
# ---- Combine and Filter Metrics ----
all_results <- bind_rows(
collect_metrics(logistic_fit) |> mutate(model = "Logistic"),
collect_metrics(dtree_fit)    |> mutate(model = "Decision Tree"),
collect_metrics(knn_fit)      |> mutate(model = "KNN"),
collect_metrics(nbayes_fit)   |> mutate(model = "Naive Bayes"),
collect_metrics(rulefit_fit)  |> mutate(model = "RuleFit")
)
# ---- Keep only the best score per model per metric ----
metrics_to_keep <- c("accuracy", "precision", "recall", "f_meas", "roc_auc")
summary_metrics <- all_results |>
filter(.metric %in% metrics_to_keep) |>
group_by(model, .metric) |>
slice_max(mean, with_ties = FALSE) |>
ungroup() |>
select(model, .metric, mean, std_err)
# ---- Pivot to Wide Format and Clean ----
summary_clean <- summary_metrics |>
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
) |>
rename_with(~ str_replace(., "_mean$", ""), ends_with("_mean")) |>
rename_with(~ str_replace(., "_std_err$", "std_err"), ends_with("_std_err")) |>
select(model, accuracy, precision, recall, f_meas, roc_auc, f_measstd_err) |>
rename(std_err_f_meas = f_measstd_err)
# ---- Sort by F1-score ----
summary_final_attempt5 <- summary_clean |>
arrange(desc(f_meas))
# ---- Preview Final Table ----
print(summary_final_attempt5)
# ---- Save RDA ----
save(summary_final_attempt5, file = here("attempt_5/results/attempt_5_summary_table.rda"))
# =============================
# Analysis: Attempt 1 Summary Table
# =============================
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(doMC)
library(pROC)
tidymodels_prefer()
set.seed(301)
Analysis: Attempt 1 Summary Table
library(tidyverse)
library(tidymodels)
library(here)
library(doMC)
library(pROC)
tidymodels_prefer()
set.seed(301)
load(here("attempt_1/results/data_split.rda"))
load(here("attempt_1/recipes/ks_recipes.rda"))
load(here("attempt_1/results/a_logistic_fit.rda"))
load(here("attempt_1/results/a_dtree_fit.rda"))
load(here("attempt_1/results/a_nbayes_fit.rda"))
load(here("attempt_1/results/a_knn_fit.rda"))
load(here("attempt_1/results/a_rulefit_fit.rda"))
# ---- Combine and Filter Metrics ----
all_results <- bind_rows(
collect_metrics(logistic_fit_a) |> mutate(model = "Logistic"),
collect_metrics(dtree_fit_a)    |> mutate(model = "Decision Tree"),
collect_metrics(knn_fit_a)      |> mutate(model = "KNN"),
collect_metrics(nbayes_fit_a)   |> mutate(model = "Naive Bayes"),
collect_metrics(rulefit_fit_a)  |> mutate(model = "RuleFit")
)
# ---- Combine and Filter Metrics ----
all_results <- bind_rows(
collect_metrics(logistic_fit) |> mutate(model = "Logistic"),
collect_metrics(dtree_fit_a)    |> mutate(model = "Decision Tree"),
collect_metrics(knn_fit_a)      |> mutate(model = "KNN"),
collect_metrics(nbayes_fit_a)   |> mutate(model = "Naive Bayes"),
collect_metrics(rulefit_fit_a)  |> mutate(model = "RuleFit")
)
# ---- Combine and Filter Metrics ----
all_results <- bind_rows(
collect_metrics(logistic_fit) |> mutate(model = "Logistic"),
collect_metrics(dtree_fit)    |> mutate(model = "Decision Tree"),
collect_metrics(knn_fit)      |> mutate(model = "KNN"),
collect_metrics(nbayes_fit)   |> mutate(model = "Naive Bayes"),
collect_metrics(rulefit_fit)  |> mutate(model = "RuleFit")
)
# ---- Keep only the best score per model per metric ----
metrics_to_keep <- c("accuracy", "precision", "recall", "f_meas", "roc_auc")
summary_metrics <- all_results |>
filter(.metric %in% metrics_to_keep) |>
group_by(model, .metric) |>
slice_max(mean, with_ties = FALSE) |>
ungroup() |>
select(model, .metric, mean, std_err)
# ---- Pivot to Wide Format and Clean ----
summary_clean <- summary_metrics |>
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
) |>
rename_with(~ str_replace(., "_mean$", ""), ends_with("_mean")) |>
rename_with(~ str_replace(., "_std_err$", "std_err"), ends_with("_std_err")) |>
select(model, accuracy, precision, recall, f_meas, roc_auc, f_measstd_err) |>
rename(std_err_f_meas = f_measstd_err)
# ---- Sort by F1-score ----
summary_final_attempt1 <- summary_clean |>
arrange(desc(f_meas))
# ---- Preview Final Table ----
print(summary_final_attempt1)
# ---- Save to CSV and RDA ----
save(summary_final_attempt1, file = here("attempt_1/results/attempt_1_summary_table.rda"))
summary_metrics <- all_results |>
filter(.metric %in% metrics_to_keep) |>
group_by(model, .metric) |>
slice_max(mean, with_ties = FALSE) |>
ungroup() |>
select(model, .metric, mean, std_err)
# ---- Pivot to Wide Format and Clean ----
summary_clean <- summary_metrics |>
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
) |>
rename_with(~ str_replace(., "_mean$", ""), ends_with("_mean")) |>
rename_with(~ str_replace(., "_std_err$", "std_err"), ends_with("_std_err")) |>
select(model, accuracy, precision, recall, f_meas, roc_auc, f_measstd_err) |>
rename(std_err_f_meas = f_measstd_err)
# ---- Sort by F1-score ----
summary_final_attempt1 <- summary_clean |>
arrange(desc(f_meas))
# ---- Preview Final Table ----
print(summary_final_attempt1)
# ---- Save to CSV and RDA ----
save(summary_final_attempt1, file = here("attempt_1/results/attempt_1_summary_table.rda"))
# =============================
# Analysis: Attempt 3 Summary Table
# =============================
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(doMC)
library(pROC)
tidymodels_prefer()
set.seed(301)
# ---- Register Parallel Processing ----
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# ---- Load Data and Fits ----
load(here("attempt_3/results/data_split.rda"))
load(here("attempt_3/results/c_logistic_fit.rda"))
load(here("attempt_3/results/c_dtree_fit.rda"))
load(here("attempt_3/results/c_nbayes_fit.rda"))
load(here("attempt_3/results/c_knn_fit.rda"))
load(here("attempt_3/results/c_rulefit_fit.rda"))
# ---- Combine and Filter Metrics ----
all_results <- bind_rows(
collect_metrics(logistic_fit) |> mutate(model = "Logistic"),
collect_metrics(dtree_fit)    |> mutate(model = "Decision Tree"),
collect_metrics(knn_fit)      |> mutate(model = "KNN"),
collect_metrics(nbayes_fit)   |> mutate(model = "Naive Bayes"),
collect_metrics(rulefit_fit)  |> mutate(model = "RuleFit")
)
# ---- Combine and Filter Metrics ----
all_results <- bind_rows(
collect_metrics(logistic_fit_c) |> mutate(model = "Logistic"),
collect_metrics(dtree_fit_c)    |> mutate(model = "Decision Tree"),
collect_metrics(knn_fit_c)      |> mutate(model = "KNN"),
collect_metrics(nbayes_fit_c)   |> mutate(model = "Naive Bayes"),
collect_metrics(rulefit_fit_c)  |> mutate(model = "RuleFit")
)
# ---- Keep only the best score per model per metric ----
metrics_to_keep <- c("accuracy", "precision", "recall", "f_meas", "roc_auc")
summary_metrics <- all_results |>
filter(.metric %in% metrics_to_keep) |>
group_by(model, .metric) |>
slice_max(mean, with_ties = FALSE) |>
ungroup() |>
select(model, .metric, mean, std_err)
# ---- Pivot to Wide Format and Clean ----
summary_clean <- summary_metrics |>
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
) |>
rename_with(~ str_replace(., "_mean$", ""), ends_with("_mean")) |>
rename_with(~ str_replace(., "_std_err$", "std_err"), ends_with("_std_err")) |>
select(model, accuracy, precision, recall, f_meas, roc_auc, f_measstd_err) |>
rename(std_err_f_meas = f_measstd_err)
# ---- Sort by F1-score ----
summary_final_attempt3 <- summary_clean |>
arrange(desc(f_meas))
# ---- Preview Final Table ----
print(summary_final_attempt3)
# ---- Save RDA ----
save(summary_final_attempt3, file = here("attempt_3/results/attempt_3_summary_table.rda"))
summary_final_attempt5
summary_final_attempt2
summary_final_attempt1
summary_final_attempt3
summary_final_attempt4
summary_final_attempt2
summary_final_attempt5
summary_final_attempt1
gc()
summary_final_attempt4
summary_final_attempt2
gc()
summary_final_attempt4
# =============================
# Analysis: Attempt 4 Summary Table
# =============================
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(doMC)
library(pROC)
tidymodels_prefer()
set.seed(301)
# ---- Register Parallel Processing ----
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# ---- Load Data and Fits ----
load(here("attempt_4/results/data_split.rda"))
load(here("attempt_4/results/d_logistic_fit.rda"))
load(here("attempt_4/results/d_dtree_fit.rda"))
load(here("attempt_4/results/d_nbayes_fit.rda"))
load(here("attempt_4/results/d_knn_fit.rda"))
load(here("attempt_4/results/d_rulefit_fit.rda"))
# ---- Combine and Filter Metrics ----
all_results <- bind_rows(
collect_metrics(logistic_fit_d) |> mutate(model = "Logistic"),
collect_metrics(dtree_fit_d)    |> mutate(model = "Decision Tree"),
collect_metrics(knn_fit_d)      |> mutate(model = "KNN"),
collect_metrics(nbayes_fit_d)   |> mutate(model = "Naive Bayes"),
collect_metrics(rulefit_fit_d)  |> mutate(model = "RuleFit")
)
# ---- Keep only the best score per model per metric ----
metrics_to_keep <- c("accuracy", "precision", "recall", "f_meas", "roc_auc")
summary_metrics <- all_results |>
filter(.metric %in% metrics_to_keep) |>
group_by(model, .metric) |>
slice_max(mean, with_ties = FALSE) |>
ungroup() |>
select(model, .metric, mean, std_err)
# ---- Pivot to Wide Format and Clean ----
summary_clean <- summary_metrics |>
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
) |>
rename_with(~ str_replace(., "_mean$", ""), ends_with("_mean")) |>
rename_with(~ str_replace(., "_std_err$", "std_err"), ends_with("_std_err")) |>
select(model, accuracy, precision, recall, f_meas, roc_auc, f_measstd_err) |>
rename(std_err_f_meas = f_measstd_err)
# ---- Sort by F1-score ----
summary_final_attempt4 <- summary_clean |>
arrange(desc(f_meas))
# ---- Preview Final Table ----
print(summary_final_attempt4)
collect_predictions(dtree_fit_d) |>
group_by(id) |>
summarise(n_preds = n_distinct(.pred_class))
dtree_fit_d <- tune_grid(
dtree_workflow,
resamples = data_fold,        # Assuming `data_fold` is your cross-validation folds
grid = dtree_grid,
control = control_grid(save_workflow = TRUE)
)
# ---- Preview Final Table ----
print(summary_final_attempt2)
# =============================
# Analysis: Attempt 4 Summary Table
# =============================
# ---- Load Libraries ----
library(tidyverse)
library(tidymodels)
library(here)
library(doMC)
library(pROC)
tidymodels_prefer()
set.seed(301)
# ---- Register Parallel Processing ----
registerDoMC(cores = parallel::detectCores(logical = TRUE))
# ---- Load Data and Fits ----
load(here("attempt_4/results/data_split.rda"))
load(here("attempt_4/results/d_logistic_fit.rda"))
load(here("attempt_4/results/d_dtree_fit.rda"))
load(here("attempt_4/results/d_nbayes_fit.rda"))
load(here("attempt_4/results/d_knn_fit.rda"))
load(here("attempt_4/results/d_rulefit_fit.rda"))
# ---- Combine and Filter Metrics ----
all_results <- bind_rows(
collect_metrics(logistic_fit_d) |> mutate(model = "Logistic"),
collect_metrics(dtree_fit_d)    |> mutate(model = "Decision Tree"),
collect_metrics(knn_fit_d)      |> mutate(model = "KNN"),
collect_metrics(nbayes_fit_d)   |> mutate(model = "Naive Bayes"),
collect_metrics(rulefit_fit_d)  |> mutate(model = "RuleFit")
)
# ---- Keep only the best score per model per metric ----
metrics_to_keep <- c("accuracy", "precision", "recall", "f_meas", "roc_auc")
summary_metrics <- all_results |>
filter(.metric %in% metrics_to_keep) |>
group_by(model, .metric) |>
slice_max(mean, with_ties = FALSE) |>
ungroup() |>
select(model, .metric, mean, std_err)
# ---- Pivot to Wide Format and Clean ----
summary_clean <- summary_metrics |>
pivot_wider(
names_from = .metric,
values_from = c(mean, std_err),
names_glue = "{.metric}_{.value}"
) |>
rename_with(~ str_replace(., "_mean$", ""), ends_with("_mean")) |>
rename_with(~ str_replace(., "_std_err$", "std_err"), ends_with("_std_err")) |>
select(model, accuracy, precision, recall, f_meas, roc_auc, f_measstd_err) |>
rename(std_err_f_meas = f_measstd_err)
# ---- Sort by F1-score ----
summary_final_attempt4 <- summary_clean |>
arrange(desc(f_meas))
# ---- Preview Final Table ----
print(summary_final_attempt4)
collect_predictions(dtree_fit_d) |>
group_by(id) |>
summarise(n_preds = n_distinct(.pred_class))
library(tidyverse)
library(here)
# Load each table manually
load(here("attempt_1/results/attempt_1q_summary_table.rda"))  # summary_final_attempt1q
library(tidyverse)
library(here)
# Load each table manually
load(here("attempt_1/results/attempt_1q_summary_table.rda"))  # summary_final_attempt1q
library(tidyverse)
library(here)
# Load each table manually
load(here("attempt_1/results/attempt_1_summary_table.rda"))  # summary_final_attempt1q
load(here("attempt_2/results/attempt_2_summary_table.rda"))
load(here("attempt_3/results/attempt_3_summary_table.rda"))
load(here("attempt_4/results/attempt_4_summary_table.rda"))
load(here("attempt_5/results/attempt_5_summary_table.rda"))
# Combine into one laptop table
laptop_summary_all <- bind_rows(
summary_final_attempt1q,
summary_final_attempt2q,
summary_final_attempt3q,
summary_final_attempt4q,
summary_final_attempt5q
)
# Combine into one laptop table
laptop_summary_all <- bind_rows(
summary_final_attempt1,
summary_final_attempt2,
summary_final_attempt3,
summary_final_attempt4,
summary_final_attempt5
)
laptop_summary_all
# Save it
save(laptop_summary_all, file = here("combined_results/laptop_summary_all.rda"))
# Save it
save(laptop_summary_all, file = here("laptop_summary_all.rda"))
# Save it
save(laptop_summary_all, file = here("master_comparison/laptop_summary_all.rda"))
laptop_summary_all
View(laptop_summary_all)
library(tidyverse)
library(here)
# ---- STEP 1: Load individual attempt summary tables ----
load(here("attempt_1/results/attempt_1_summary_table.rda"))  # summary_final_attempt1
load(here("attempt_2/results/attempt_2_summary_table.rda"))
load(here("attempt_3/results/attempt_3_summary_table.rda"))
load(here("attempt_4/results/attempt_4_summary_table.rda"))
load(here("attempt_5/results/attempt_5_summary_table.rda"))
# ---- STEP 2: Add attempt numbers and platform info ----
summary_final_attempt1 <- summary_final_attempt1 |>
mutate(attempt = 1, platform = "Local")
summary_final_attempt2 <- summary_final_attempt2 |>
mutate(attempt = 2, platform = "Local")
summary_final_attempt3 <- summary_final_attempt3 |>
mutate(attempt = 3, platform = "Local")
summary_final_attempt4 <- summary_final_attempt4 |>
mutate(attempt = 4, platform = "Local")
summary_final_attempt5 <- summary_final_attempt5 |>
mutate(attempt = 5, platform = "Local")
# ---- STEP 3: Combine all attempts into one table ----
local_summary_all <- bind_rows(
summary_final_attempt1,
summary_final_attempt2,
summary_final_attempt3,
summary_final_attempt4,
summary_final_attempt5
)
local_summary_all
# ---- STEP 4: Clean model names and remove missing values ----
local_summary_all <- local_summary_all |>
mutate(model = str_to_title(model)) |>
drop_na(f_meas)
local_summary_all
# ---- STEP 5: Optional Summary Table ----
local_model_summary <- local_summary_all |>
group_by(model, platform) |>
summarise(
mean_f1 = mean(f_meas),
sd_f1 = sd(f_meas),
mean_accuracy = mean(accuracy),
sd_accuracy = sd(accuracy),
.groups = "drop"
) |>
arrange(desc(mean_f1))
local_model_summary
local_summary_all
library(tidyverse)
library(here)
# ---- STEP 1: Load attempt summary tables ----
load(here("attempt_1/results/attempt_1_summary_table.rda"))  # summary_final_attempt1
load(here("attempt_2/results/attempt_2_summary_table.rda"))
load(here("attempt_3/results/attempt_3_summary_table.rda"))
load(here("attempt_4/results/attempt_4_summary_table.rda"))
load(here("attempt_5/results/attempt_5_summary_table.rda"))
# ---- STEP 2: Define recipe labels for each attempt (customize these) ----
recipe_labels <- c(
"Kitchen Sink",
"M2",
"M3",
"M1",
"M4"
)
# ---- STEP 3: Add metadata to each attempt ----
summary_final_attempt1 <- summary_final_attempt1 |>
mutate(attempt = 1, platform = "Local", recipe = recipe_labels[1])
summary_final_attempt2 <- summary_final_attempt2 |>
mutate(attempt = 2, platform = "Local", recipe = recipe_labels[2])
summary_final_attempt3 <- summary_final_attempt3 |>
mutate(attempt = 3, platform = "Local", recipe = recipe_labels[3])
summary_final_attempt4 <- summary_final_attempt4 |>
mutate(attempt = 4, platform = "Local", recipe = recipe_labels[4])
summary_final_attempt5 <- summary_final_attempt5 |>
mutate(attempt = 5, platform = "Local", recipe = recipe_labels[5])
# ---- STEP 4: Combine all into one table ----
local_summary_all <- bind_rows(
summary_final_attempt1,
summary_final_attempt2,
summary_final_attempt3,
summary_final_attempt4,
summary_final_attempt5
)
local_summary_all
# ---- STEP 5: Clean model names and remove rows with missing metrics ----
local_summary_all <- local_summary_all |>
mutate(model = str_to_title(model)) |>
drop_na(f_meas)
local_summary_all
View(local_model_summary)
View(local_summary_all)
# ---- STEP 5: Create best model per attempt table (based on F1-score) ----
local_best_models <- local_summary_all |>
group_by(attempt) |>
filter(f_meas == max(f_meas, na.rm = TRUE)) |>
ungroup() |>
arrange(attempt)
local_best_models
library(tidyverse)
library(here)
# ---- STEP 1: Load attempt summary tables ----
load(here("attempt_1/results/attempt_1_summary_table.rda"))  # summary_final_attempt1
load(here("attempt_2/results/attempt_2_summary_table.rda"))
load(here("attempt_3/results/attempt_3_summary_table.rda"))
load(here("attempt_4/results/attempt_4_summary_table.rda"))
load(here("attempt_5/results/attempt_5_summary_table.rda"))
# ---- STEP 2: Define recipe labels for each attempt (customize these) ----
recipe_labels <- c(
"Kitchen Sink",
"M2",
"M3",
"M1",
"M4"
)
# ---- STEP 3: Add metadata to each attempt ----
summary_final_attempt1 <- summary_final_attempt1 |>
mutate(attempt = 1, platform = "Local", recipe = recipe_labels[1])
summary_final_attempt2 <- summary_final_attempt2 |>
mutate(attempt = 2, platform = "Local", recipe = recipe_labels[2])
summary_final_attempt3 <- summary_final_attempt3 |>
mutate(attempt = 3, platform = "Local", recipe = recipe_labels[3])
summary_final_attempt4 <- summary_final_attempt4 |>
mutate(attempt = 4, platform = "Local", recipe = recipe_labels[4])
summary_final_attempt5 <- summary_final_attempt5 |>
mutate(attempt = 5, platform = "Local", recipe = recipe_labels[5])
# ---- STEP 4: Combine all into one table ----
local_summary_all <- bind_rows(
summary_final_attempt1,
summary_final_attempt2,
summary_final_attempt3,
summary_final_attempt4,
summary_final_attempt5
)
# ---- STEP 5: Create best model per attempt table (based on F1-score) ----
local_best_models <- local_summary_all |>
group_by(attempt) |>
filter(f_meas == max(f_meas, na.rm = TRUE)) |>
ungroup() |>
arrange(attempt)
# ---- STEP 6: (Optional) Save cleaned full dataset ----
save(local_summary_all, local_best_models, file = here("master_comparison/local_model_results.rda"))
library(tidyverse)
library(here)
# ---- STEP 1: Load Quest summary tables ----
load(here("attempt_1/results/attempt_1q_summary_table.rda"))  # summary_final_attempt1q
