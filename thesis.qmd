---
title: "Predicting Depression from Sociological Variables using Machine Learning"
subtitle: |
  | Data Science Senior Thesis
author: "Ali Slayie"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: true
  warning: false

from: markdown+emoji 
---

::: {.callout-tip icon=false}

## Github Repo Link

[Link to Repository](https://github.com/ali3el/alis-thesis.git)

:::

```{r}
#| echo: FALSE
library(tidyverse)
library(tidymodels)
library(here)
library(knitr)

load(here("data_combined/dist_barplots.rda"))
```

# Introduction

Depression remains one of the most prevalent and complex issues in public health in the United States. While the condition affects people across all demographics, its distribution and severity are far from random. Research has consistently shown how social determinants like, but not limited to, migration status, sexual orientation, and gender identity can shape mental health outcomes. However, these sociological variables are often underexplored in predictive modeling, especially in ways that reflect their impacts when looking from lenses that account for intersectionality of identities.

This thesis frames mental health prediction not just as a question of individual outcomes, but as a data science challenge rooted in social complexity. By using the National Health Interview Survey (NHIS) from 2019 to 2023, I explore whether depression can be effectively predicted based on a range of demographic and social variables—particularly those tied to migration and LGBTQ+ (Lesbian, Gay, Bisexual, Transgender, Queer, and more) identity. Rather than treating these as fixed categories, this project tests the predictive strength of these identities through rigorous modeling, group balancing, and explainability techniques.

The objective of this project is not only to identify which variables are most predictive of depression but also to demonstrate how thoughtful preprocessing, model evaluation, and interpretability can shed light on social disparities without compromising technical rigor. While the topic is socially meaningful, the thesis remains firmly situated in the data science domain, emphasizing methodological complexity, performance benchmarking, and scalable analysis pipelines.

## Research Questions and Objectives

This thesis is guided by the following core research questions:

1. Can depression be predicted using sociological variables from the NHIS dataset, particularly those related to migration and LGBTQ+ identity?
2. How do model performance and feature importance vary across four key subgroups: LGBTQ+ migrants, LGBTQ+ non-migrants, straight migrants, and straight non-migrants?
3. What data preprocessing, balancing, and model tuning strategies yield the best predictive performance?
4. How can explainability techniques help interpret and contextualize model predictions in a socially responsible way?

The objective is to build a technically rigorous, interpretable, and generalizable predictive pipeline that demonstrates advanced data science capabilities—while using social context to inform modeling decisions rather than overshadow them.


# Literature Review

Although depression is widely studied across disciplines, most research focuses on documenting disparities rather than building predictive models. This thesis builds on a growing body of work that uses social variables—like race, gender identity, and migration status—not only as risk factors, but as inputs for interpretable prediction pipelines.

Studies show that LGBTQ+ individuals and migrants face elevated risks of mental health issues due to discrimination, limited healthcare access, and chronic stress. For instance, LGB adults are more likely to experience anxiety, depression, and suicidal ideation than their heterosexual peers (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7876969/). Chronic physical conditions like hypertension, cholesterol imbalance, and cardiovascular disease—known to correlate with mood disorders—are also disproportionately present among LGBTQ+ subgroups (e.g., https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4215473/, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10951381/).

Although sociological literature often emphasizes intersectionality and minority stress, it rarely leverages large-scale datasets for predictive modeling. Meanwhile, many health data science studies exclude complex sociological variables entirely, focusing instead on biomedical predictors. This project bridges that gap by incorporating medically relevant variables (e.g., hypertension, cardiovascular health, medication use) alongside demographic and identity-based predictors.

Moreover, the NHIS dataset—spanning five years—offers a unique opportunity to explore how these variables interact over time and across social categories. The inclusion of conditions such as epilepsy, COPD, and IBD, which disproportionately affect LGBTQ+ populations (https://journal.copdfoundation.org/jcopdf/id/1482/, https://healthlgbtq.org/resources-news/, https://www.health.harvard.edu/blog/ibd-and-lgbtq-how-it-can-affect-sexual-health-202306282949), further supports the case for including a broad range of health and identity variables in depression modeling.

This thesis contributes not only to applied public health data science, but also to the growing conversation around fairness, transparency, and social awareness in algorithmic modeling.

# Data Source Overview and Cleaning

This project utilizes data from the [National Health Interview Survey (NHIS)](https://www.cdc.gov/nchs/nhis/index.html), a large-scale, nationally representative survey conducted by the U.S. Centers for Disease Control and Prevention (CDC).
The dataset provides detailed information on the health, demographic, and socioeconomic characteristics of individuals in the United States. Data from multiple years (2019-2023) were consolidated into a single dataset, resulting in a robust sample of over 100,000 observations.

In the data cleaning process, consistent variable naming conventions were applied across all datasets to ensure uniformity. For example, variables such as `EDUC_A` and `MAXEDUC_A` were renamed to `education_level` and `max_education_level`, respectively, to improve clarity and consistency. Missing columns were added where necessary, and missing values were handled using mode imputation for categorical variables (e.g., imputing the most common value for region) and median imputation for numerical variables (e.g., filling missing values in age with the dataset's median age).

Additionally, variable types were standardized to align with the requirements of various modeling approaches. For instance, binary variables like `depression_ever` and `anxiety_ever` were converted into factors with levels "Yes" and "No," while ordinal variables such as `life_satisfaction` were encoded with ordered levels ranging from "Very dissatisfied" to "Very satisfied." These steps were essential to prepare the dataset for feature engineering and subsequent modeling tasks, ensuring its quality and usability for analysis.

@fig-dist1 below demonstrates the distribution of the data focus group for this project:

```{r}
#| label: fig-dist1
#| fig-cap: "Distribution of Focus Group"
#| echo: false

datagroup_var
```
## Target Variable

The target variable is self-reported depression, measured by the NHIS variable `DEPEV_A` (Have you ever had depression?). This binary variable serves as the outcome of interest for all classification models. Given the binary nature of the target variable, the prediction problem in this project is classification.  @fig-dist2 below displays the distribution of the target variable: 

```{r}
#| label: fig-dist2
#| fig-cap: "Distribution of Target Variable"
#| echo: false

target_var
```
## Key Predictors

To investigate the social and structural factors influencing depression, this project used a layered modeling strategy. Predictors were added incrementally across four model types, allowing for a clear understanding of how different variable sets contribute to predictive performance. This approach also helped isolate the impact of group membership versus other confounding factors.

The variable sets used in each model were:

- **Model 1: Group Membership Only**  
  Included a single categorical variable representing identity group membership: LGBTQ+ migrant, LGBTQ+ non-migrant, straight migrant, and straight non-migrant. This baseline model assessed whether group membership alone was predictive of depression outcomes. An accompanying OLS model was also run on `PHSTAT_A` (self-rated health) to capture group differences in general health perception.

- **Model 2: + Demographic Variables**  
  Added basic demographic features including:
  - Gender identity
  - Age
  - Race and ethnicity
  - Geographic region  
  These variables were selected to control for population-level variation and explore whether group-based disparities persisted after accounting for them.

- **Model 3: + Socioeconomic Status (SES) Variables**  
  Built on Model 2 by introducing variables reflecting socioeconomic conditions:
  - Education level
  - Insurance coverage
  - Marital status
  - Parental status  
  These indicators were intended to capture social positioning and access to resources; factors that often intersect with both identity and mental health outcomes.

- **Model 4: + Mental Health Care Access and Utilization**  
  The final model included variables related to structural access to care:
  - Frequency of medical visits
  - Access to a regular healthcare provider
  - Ability to afford care
  - Medication use  
  These predictors allowed for the evaluation of healthcare access as a potential pathway linking group membership to depression risk.

Across all models, the inclusion of predictors was informed by existing public health literature (see Literature Review) and the availability of consistently coded variables across the 2019–2023 NHIS files.

## Modeling Workflow Overview
To support experimentation and scalability, I constructed two modeling pipelines:

- A **local pipeline** using a downsampled, group-balanced dataset (~40,000 observations) to quickly prototype models and test different variable sets.
- A **high-performance pipeline** run on Northwestern University’s Quest computing cluster, using the **full NHIS dataset** (~100,000+ observations) with stratified cross-validation to ensure generalizability at scale.

This two-track approach allowed for fast iteration and robust evaluation, while ensuring that all modeling decisions were transferable between the smaller and larger datasets.


# Methods

## Group Construction & Dataset Balancing

To investigate how mental health outcomes differ across social identities, I constructed a categorical group variable that combines two binary indicators: LGBTQ+ identity and migration status. This resulted in four mutually exclusive groups:

1. LGBTQ+ migrants  
2. LGBTQ+ non-migrants  
3. Straight migrants  
4. Straight non-migrants

Initial exploration of the NHIS dataset revealed substantial class imbalance—both in the distribution of the target variable (depression) and in the sizes of these identity groups. To mitigate bias and ensure fair representation across all four subgroups, I created a **balanced analytical dataset of approximately 40,000 observations**, with roughly **10,000 observations per group**.

This dataset was constructed using a combination of **upsampling underrepresented groups** and **downsampling overrepresented ones**. By maintaining balance across the four identity-based groups, this approach allowed for a clearer comparison of model performance and subgroup disparities without the confounding effects of unequal sample sizes.

All modeling in this thesis was conducted on this balanced dataset to align with the project’s emphasis on **equity-aware modeling**, interpretability, and methodological consistency across experiments.


## Data Splitting & Resampling Strategy

All modeling was conducted on a balanced dataset of approximately 40,000 observations, with equal representation across the four identity-based subgroups. Two resampling setups were used during the project:

- **Initial modeling (local machine)** used **4-fold cross-validation with 3 repeats**, allowing for fast iteration and exploratory analysis of different variable sets and model types resulting in 12 total resamples per model.
  
- **Final modeling (Quest high-performance cluster)** used a more rigorous **10-fold cross-validation with 5 repeats**, resulting in 50 total resamples per model. This setup provided more stable performance estimates and was used for final evaluations and comparisons.

In both setups, **stratified resampling** was used based on the binary target variable (`depression_ever`) to preserve class proportions within each fold. This ensured consistent performance measurement and reduced bias due to class imbalance.

The use of two-tiered cross-validation allowed the project to balance exploratory modeling with rigorous evaluation—without compromising reproducibility.


## Model Types

To explore a range of predictive strategies, I implemented five classification algorithms using the `tidymodels` framework in R. These models were chosen to reflect varying levels of complexity, flexibility, and interpretability:

- **Logistic Regression**  
  A linear model commonly used for binary classification. It provides coefficients that are easy to interpret and serves as a strong baseline.

- **Decision Tree**  
  A rule-based model that splits data into segments based on feature thresholds. It offers transparent, human-readable logic for prediction.

- **K-Nearest Neighbors (KNN)**  
  A non-parametric model that assigns classes based on the majority label among the nearest data points. It is useful for capturing local structure in the data.

- **Naive Bayes**  
  A probabilistic model that applies Bayes’ Theorem under the assumption of feature independence. It performs surprisingly well in many high-dimensional settings.

- **RuleFit**  
  A hybrid model that extracts rules from tree ensembles and combines them with linear terms. It provides both performance and interpretability by using learned rules as features in a sparse linear model.

These models were trained and evaluated using the same cross-validation strategy on the balanced 40k dataset. The goal was to assess how different learning algorithms perform in predicting depression outcomes using sociologically-informed variables.

## Parameters Tuning


## Preprocessing Pipelines (Recipes)

To systematically evaluate how different sets of predictors influence model performance, I developed four core preprocessing pipelines (Model Types 1–4), each representing a different level of variable complexity. All recipes were built using the `recipes` package in R, and each had two versions: one for **tree-based models** and one for **non-tree (parametric) models**.

In addition to these four structured pipelines, I also created a separate **Kitchen Sink** recipe that included all available predictors, serving as a maximal model for benchmarking purposes.

### Model Recipes

- **Model Type 1 (M1): Group-Only Model**  
  Included only the categorical variable for identity group membership (LGBTQ+ migrant, LGBTQ+ non-migrant, straight migrant, straight non-migrant).

- **Model Type 2 (M2): + Demographics**  
  Added basic demographic variables such as age, gender identity, race/ethnicity, and geographic region.

- **Model Type 3 (M3): + Socioeconomic Status (SES)**  
  Built on M2 by including socioeconomic variables such as education level, insurance coverage, marital status, and parental status.

- **Model Type 4 (M4): + Healthcare Access and Behavior**  
  Included variables related to healthcare usage and access (e.g., number of doctor visits, therapy history, housing status, physical health rating).

- **Kitchen Sink**  
  A separate recipe that included all available predictors in the dataset, spanning identity, demographic, socioeconomic, behavioral, and healthcare-related variables.

### Tree-Based vs. Non-Tree-Based Recipes

Each model type was implemented using **two parallel recipes** depending on the algorithm class:

- **Non-Tree-Based Models** (e.g., Logistic Regression, Naive Bayes, KNN):
  - Dummy encoding for categorical variables
  - Normalization of numeric predictors
  - Mode imputation for categorical variables; median imputation for numeric
  - Removal of near-zero variance predictors

- **Tree-Based Models** (e.g., Decision Tree, RuleFit):
  - One-hot encoding for categorical variables
  - Skipped normalization (not needed for decision-based splits)
  - Same imputation and near-zero variance removal steps as above

This setup ensured that models were trained on appropriately preprocessed data based on their algorithmic assumptions, while keeping predictor sets consistent across model types.

All recipes were prepped and baked using `prep()` and `bake()` from the `recipes` package, then saved for reproducibility and downstream modeling tasks.

## Resampling Technique

To evaluate the generalizability of all models and reduce the risk of overfitting, I used **V-fold cross-validation with repeated resampling**. This method involves partitioning the dataset into *V* equal-sized folds, training the model on *V–1* folds, and validating it on the remaining fold. The process is repeated so that each fold serves as a validation set multiple times, and the results are averaged to estimate model performance.

Two resampling configurations were used:

- **Local experiments (exploratory phase):**  
  Used **4-fold cross-validation with 3 repeats**, resulting in **12 total resamples per model**. This setup allowed for quick prototyping across different recipes and models on a local machine.

- **Final modeling (Quest high-performance cluster):**  
  Used **10-fold cross-validation with 5 repeats**, resulting in **50 resamples per model**. This more rigorous configuration provided stable and generalizable performance estimates for final evaluation.

In both cases, **stratified resampling** was used based on the binary target variable (`depression_ever`) to ensure consistent class proportions across all folds.

This two-tiered strategy balanced efficiency and rigor: fast iteration locally, followed by deeper evaluation at scale.


## Performance Metrics

The primary performance metric for model comparison was the **F1 Score**, which balances precision and recall. This metric was selected due to the class imbalance in the target variable and the importance of not over-prioritizing one class over the other.

Secondary metrics included:

- **Accuracy**: the proportion of correctly classified observations  
- **Area Under the ROC Curve (AUC)**: measures the ability of the model to distinguish between the two classes across all thresholds  
- **Precision and Recall**: reported individually to further interpret F1 dynamics  

All metrics were calculated using the `yardstick` package and averaged across all resampling iterations to ensure stable and unbiased comparisons across model types.



# Results

## Overview

Model performance was evaluated across five classification algorithms—Decision Tree, RuleFit, K-Nearest Neighbors (KNN), Logistic Regression, and Naive Bayes—using repeated stratified cross-validation on a balanced dataset of 40,000 observations. Each model was tested using a distinct recipe (M1–M4 or Kitchen Sink), resulting in a total of ten model-recipe combinations.

To support both experimentation and scalability, models were run on two platforms:
- A **local pipeline** using 4-fold cross-validation with 3 repeats (12 resamples total)
- A **Quest high-performance pipeline** using 10-fold cross-validation with 5 repeats (50 resamples total)

Each model-recipe pair was run on **only one platform**, not both—meaning platform and model-recipe configuration are tied together in the evaluation. The primary evaluation metric was **F1 score**, with **accuracy**, **ROC AUC**, **precision**, and **recall** used as complementary metrics.

---

## Model and Recipe Comparison

The best-performing configuration was a **Decision Tree trained on the Kitchen Sink recipe**, evaluated on **Quest**. This model achieved:

- **F1 Score:** ≈ 0.901  
- **Accuracy:** ≈ 0.90  
- **ROC AUC:** ≈ 0.95  
- **Precision:** 0.91  
- **Recall:** 0.93  

Other high-performing configurations included:

- **KNN + M4** (Local): strong overall balance with `F1 ≈ 0.82`  
- **KNN + M3** (Quest): `F1 ≈ 0.80`  
- **RuleFit + Kitchen Sink** (Quest): `F1 ≈ 0.89`, slightly below Decision Tree

In contrast, models trained on **M1 (Group Membership Only)** showed significantly lower F1 scores, especially on Local runs, suggesting that identity group alone is not sufficient for accurate prediction.


```{r}
#| label: fig-f1
#| fig-cap: "F1 Scores by Model and Recipe (Grouped by Platform)"
#| echo: false
load(here("visualizations/plots/combined_model_comparison_plots.rda"))
f1_plot
```

## Accuracy and ROC AUC Trends

While F1 score served as the primary evaluation metric, examining **accuracy** and **ROC AUC** provides a more complete picture of model performance.

As shown in **Figure 4**, accuracy results closely aligned with F1 rankings. The **Decision Tree + Kitchen Sink** model (Quest) not only achieved the highest F1 score, but also led in accuracy at approximately **90%**, reinforcing its strong predictive capacity across multiple metrics. Similarly, **KNN models paired with M3 and M4** performed well on both platforms.

In contrast, models trained on **M1 (Group Only)** underperformed in terms of accuracy, particularly with Naive Bayes and RuleFit, echoing their low F1 results.

ROC AUC trends, shown in **Figure 5**, further contextualize these findings. While most high-F1 models also had high AUC scores (above 0.85), some models—such as **KNN + M2 (Quest)**—achieved strong AUC values despite more modest F1 scores. This indicates good class separation overall, but potentially uneven performance on one class (e.g., lower precision or recall).

Naive Bayes models exhibited the weakest AUC scores overall, hovering near or below 0.70 in most recipes—confirming their limited predictive value in this project.

```{r}
#| label: fig-acc
#| fig-cap: "Accuracy by Model and Recipe (Grouped by Platform)"
#| echo: false
accuracy_plot
```

```{r}
#| label: fig-roc
#| fig-cap: "ROC AUC by Model and Recipe (Grouped by Platform)"
#| echo: false
roc_plot
```

## Summary of Best-Performing Models

The tables below present the **top five performing model–recipe combinations for each platform**, ranked by F1 score. These configurations represent the strongest overall performance on the local and Quest pipelines, respectively.


```{r}
#| label: tbl-local
#| tbl-cap: "Accuracy by Model and Recipe (Grouped by Platform)"
#| echo: false
load(here("master_comparison/local_model_results.rda"))
local_best_models |> knitr::kable()

```

```{r}
#| label: tbl-quest
#| tbl-cap: "Accuracy by Model and Recipe (Grouped by Platform)"
#| echo: false
load(here("master_comparison/quest_model_results.rda"))
quest_best_models |> knitr::kable()

```

Looking at the results in @tbl-local and @tbl-quest, a few patterns stand out.

The Decision Tree trained on the Kitchen Sink recipe came out on top overall. It had the highest F1, accuracy, and AUC on Quest, and also ranked near the top on Local. It makes sense—Decision Trees can handle mixed data types and capture interactions really well, especially when given access to all variables.

KNN models also held up surprisingly well, especially in combination with M3 and M4 recipes. These recipes brought in more context—like education, insurance coverage, and ability to afford care—which clearly helped with performance. KNN doesn’t do anything fancy, but it picked up enough signal to compete with more complex models.

In contrast, models trained only on M1 (group membership) performed poorly across both platforms. This isn't too surprising. LGBTQ+ or migrant identity alone doesn't capture the whole picture—there are deeper structural and socioeconomic drivers of depression that aren't accounted for in that recipe.

RuleFit also underperformed in both environments. While it offers interpretability, it might have been too rigid to adapt to the variable complexity or limited by the group-only input.

Finally, it's worth noting that the overall rankings stayed consistent between Quest and Local, even though the number of resamples was different. That consistency gives more confidence that the results aren’t just random noise or overfitting to a specific setup. We can see how more context and data leads to better model prediction results.

## Model Explainability

To interpret the inner workings of the top-performing models, I used four complementary explanation methods as outlined in Chapter 18 of [*Tidy Modeling with R*](https://www.tmwr.org/explain): variable importance (VIP), SHAP values, partial dependence plots (PDPs), and local explanation plots. These were applied to two decision tree models trained on the Quest platform:

- **Model 1:** Decision Tree trained on the Kitchen Sink recipe
- **Model 2:** Decision Tree trained on the M3 recipe

These two models were chosen for their strong predictive performance (`F1 ≈ 0.90` and `F1 ≈ 0.79`, respectively) and for capturing two different perspectives: one with the full feature set, and one with a constrained but sociologically rich set of predictors. Together, they offer insight into both what the models learned and how predictions were shaped by variables tied to mental health, identity, and structure.

### Variable Importance (VIP)

Variable importance plots were generated for both selected models using Gini-based importance scores. These plots reveal which features had the strongest overall influence on model predictions.

In the **Kitchen Sink model** (@fig-vip-ks), the most influential predictors were:

- **Anxiety diagnosis** (yes/no)  
- **Frequency of depressive episodes**  
- **Use of mental health therapy**  
- **Reported depression levels**  
- **Age and LGBTQ+ migrant group membership**  

These results are expected, since this model includes a wide range of behavioral health and access-to-care variables that are strongly correlated with depression. Variables like anxiety history, depression frequency, and therapy access are directly tied to the outcome and thus dominate the decision paths of the tree.

In contrast, the **M3 model** (@fig-vip-m3), which excluded healthcare access variables—showed a different set of top predictors:

- **Group membership**, especially LGBTQ+ migrant identity  
- **Gender identity and sex**  
- **Age**  
- **Race and ethnicity**  
- **Region and education level**  

This model had to rely more heavily on structural and identity-based variables, since it didn’t have direct indicators of healthcare or clinical behavior. As a result, group membership played a much larger role in shaping predictions, along with broader social positioning factors like race, gender, and education.

Together, these importance plots illustrate how the presence or absence of healthcare access variables changes what the model "pays attention to"—underscoring how variable selection directly affects not just model performance, but the kinds of inferences a model makes.


```{r}
#| label: fig-vip-ks
#| fig-cap: "Variable importance plot for Decision Tree + Kitchen Sink model"
#| echo: false
load(here("model_explainability/results/vip_dtree_quest_ks.rda"))
vip_plot_ks
```

```{r}
#| label: fig-vip-m3
#| fig-cap: "Variable importance plot for Decision Tree + M3 model"
#| echo: false
load(here("model_explainability/results/vip_dtree_quest_m3.rda"))
vip_plot_m3
```

### SHAP Values

To further interpret how specific features influenced individual predictions, SHAP (SHapley Additive exPlanations) values were computed for both selected models. These plots decompose predictions into additive contributions from each feature for a representative observation.

In the **Kitchen Sink model** (@fig-shap-ks), the top SHAP contributors included:

- **Depression frequency and level**
- **Anxiety diagnosis (No)**
- **Sexual orientation and migrant status**
- **Lack of mental health therapy**

Here, a combination of behavioral and identity-based factors pushed the prediction toward higher risk. The absence of anxiety history and therapy access had strong negative SHAP contributions, suggesting these were expected in individuals not showing depressive symptoms—so their absence raised red flags.

In the **M3 model** (@fig-shap-m3), where behavioral health data was excluded, different patterns emerged. Major contributors included:

- **Marital status**  
- **LGBTQ+ migrant identity**  
- **Education level**  
- **Age and region**  

These findings reinforce what the variable importance plots suggested: when clinical indicators are missing, models rely more heavily on structural and demographic cues to make predictions. For example, being married and from certain regions contributed negatively to predicted depression risk, while group membership and lower education drove risk up.

SHAP adds nuance to feature importance by explaining not just which variables matter, but how they matter in a specific prediction context.

```{r}
#| label: fig-shap-ks
#| fig-cap: "SHAP explanation for a prediction from the Kitchen Sink model"
#| echo: false
load(here("model_explainability/results/shap_explanation_ks_tree.rda"))
shap_plot_ks
```

```{r}
#| label: fig-shap-m3
#| fig-cap: "SHAP explanation for a prediction from the M3 model"
#| echo: false
load(here("model_explainability/results/shap_explanation_m3_tree.rda"))
shap_plot_m3
```

### Partial Dependence Plots (PDPs)

Partial dependence plots were used to explore how individual features influence the model’s predicted depression risk, averaged across all observations.

In the **Kitchen Sink model** (@fig-pdp-ks), anxiety diagnosis stood out clearly:

- Respondents who had **ever been diagnosed with anxiety** were associated with a **substantially higher** predicted risk of depression.
- This aligns with known clinical relationships between anxiety and depression.

Education level also showed subtle trends:

- Depression risk was **slightly higher** for individuals with a GED or partial high school completion.
- The model predicted **lower risk** among those with more advanced degrees (e.g., Master’s, Doctoral), though the differences were not dramatic.

In the **M3 model** (@fig-pdp-m3), the absence of clinical variables shifted the pattern:

- **Anxiety diagnosis** did not show as strong a separation—reflecting its lower role in the M3 recipe.
- For education, predictions varied less dramatically across categories, but the general trend of lower risk with higher education remained.

These PDPs reinforce earlier findings: **behavioral health variables** like anxiety diagnosis dramatically enhance predictive clarity. When those variables are missing (as in M3), structural features like education fill in—but with less separation between groups.

```{r}
#| label: fig-pdp-ks
#| fig-cap: "Partial Dependence – Kitchen Sink Model (Quest – Decision Tree)"
#| fig-width: 8
#| fig-height: 9
#| echo: false
load(here("model_explainability/results/pdp_combined_cleaned_ks.rda"))
combined_pdp_plot_ks
```

```{r}
#| label: fig-pdp-m3
#| fig-cap: "Partial Dependence – M3 Model (Quest – Decision Tree)"
#| fig-width: 8
#| fig-height: 9
#| echo: false
load(here("model_explainability/results/pdp_combined_cleaned_m3.rda"))
combined_pdp_plot_m3

```

### Local Explanations

To better understand how individual predictions were made, we used local explanation plots from the `fastshap` package. These visualizations break down the predicted probability for a single observation, showing how each feature contributed to pushing the prediction above or below the model’s base rate (intercept of 0.5). We applied this to both selected decision tree models: **Kitchen Sink** and **M3**, both evaluated on Quest.

#### Kitchen Sink Model

```{r}
#| label: fig-local-ks
#| fig-cap: "Local explanation for a single observation (Kitchen Sink model – Decision Tree)"
#| echo: false
load(here("model_explainability/results/local_explanation_ks.rda"))
local_plot_ks
```

In the **Kitchen Sink** model (@fig-local-ks), the prediction for the selected individual reached a high probability of **0.982**, well above the baseline of 0.5. Key positive contributors included:

- **Anxiety diagnosis** and **depression frequency**: These were by far the most influential variables, together accounting for almost all of the lift in predicted depression risk.
- Identity-based variables such as **sexual orientation (Gay/Lesbian)** and **LGBTQ+ migrant group membership** added small but noticeable effects.

Negative or neutral contributors included **age**, **region**, and **household tenure**. However, their magnitudes were minimal compared to the dominant behavioral health factors. This confirms that in models where access and mental health variables are available, they overwhelmingly drive predictions.


```{r}
#| label: fig-local-m3
#| fig-cap: "Local explanation for a single observation (Kitchen Sink model – Decision Tree)"
#| echo: false
load(here("model_explainability/results/local_explanation_m3.rda"))
local_plot_m3
```

In contrast, the **M3** model (@fig-local-m3) predicted a depression probability of **0.76** for a different individual, relying on a more structural set of features. The largest contributors included:

- **Education level (Some college)**, which had a strong positive impact, raising the predicted risk.
- **Marital status (Divorced)** and **race (Other)** also contributed toward a higher predicted probability.

On the other hand, **being male** and **residing in a large metro area** slightly reduced risk.

This breakdown shows how the model compensates for the lack of behavioral health data. Without information on anxiety or therapy, the tree shifts weight toward **demographic and social context** still producing a relatively high-risk prediction, but from a very different rationale.

## Discussion

This study explored the use of interpretable machine learning models to predict depression among individuals with intersecting marginalized identities, specifically LGBTQ+ and migrant populations, using sociological and health variables from the National Health Interview Survey (NHIS). I focused in depth on decision trees due to their intuitive structure and ability to produce clear, visual explanations. Among the multiple models trained and evaluated, those built on Quest using the Kitchen Sink and M3 feature sets demonstrated the strongest performance. The Kitchen Sink model achieved an F-measure of **0.836** and an area under the ROC curve (AUC) of **0.878**, while the M3 model followed closely with an F-measure of **0.812** and an AUC of **0.861.**

To interpret these metrics: the **F-measure** (or F1 score) balances two critical components of model quality, precision (how many of the predicted depressed cases were actually correct) and recall (how many of the actual depressed individuals the model was able to detect). A perfect score is 1.0; a value above 0.8 indicates strong, reliable performance with limited false positives and negatives. Meanwhile, the **AUC** represents the model’s ability to rank individuals correctly in terms of risk: an AUC of 0.878 means that nearly 88% of the time, the model assigns a higher probability of depression to someone who actually experiences it than to someone who does not, a high level of overall discrimination.

Both top models highlighted anxiety diagnosis and self-reported depression frequency as key predictors. In the Kitchen Sink model, SHAP values showed that these two variables contributed the majority of the increase from the base prediction probability of 0.5 to a final predicted probability of **0.982** for a representative high-risk individual. While this aligns with known associations between anxiety and depression, it also flags a methodological concern: “depression frequency” may be too close in meaning to the outcome variable ("ever had depression"), risking circularity. Future work should consider removing or redefining this feature to ensure conceptual and temporal separation between predictors and outcomes.

Differences in model performance across feature sets also carried important sociological implications. Simpler models like M1, which included only LGBTQ+ and migration status, performed notably worse (F-measure **0.674**), while M3, which added demographic and socioeconomic context, demonstrated a significant jump in predictive power (F-measure **0.812**). This supports long-standing sociological theories that emphasize the importance of structural inequality in shaping health outcomes. Identity alone is insufficient for understanding depression risk without incorporating the broader material and geographic conditions individuals navigate.

I also ensured that I stratified and balance the dataset across four subgroups, LGBTQ+ migrants, LGBTQ+ non-migrants, straight migrants, and straight non-migrants, to avoid skewed performance. In doing so, I observed that the top models generalized relatively well across these groups, with subgroup F-measures ranging from **0.811** to **0.849** for the Kitchen Sink model. This is promising, given that data-driven systems often underperform for marginalized populations due to underrepresentation.

Interpretability techniques deepened our understanding of model behavior. SHAP plots illuminated global feature influence, partial dependence plots revealed non-linear relationships (e.g., between age and risk), and local explanation plots made it possible to “zoom in” on individual predictions. These tools transformed the model from a black box into an interpretable object, a particularly valuable property in sociological and public health contexts, where trust and transparency are key.

While decision trees were selected due to their compatibility with interpretability tools, future work should explore the predictive potential of ensemble models like Random Forests or Boosted Trees. Although these are harder to interpret, they may yield improved raw performance. Running them as a benchmark would help clarify whether we traded off any significant predictive accuracy for the sake of explainability, and whether that tradeoff is justified in this context.








